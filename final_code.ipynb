{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 478 Final Code Submission\n",
    "Grace Wang and Didi Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import plot_tree\n",
    "import random\n",
    "from sklearn.impute import KNNImputer\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation for Morph Embeddings\n",
    "Use kNN Imputation to fill in missing pre- morph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in training data on each ADP / potential synapse\n",
    "data = pd.read_csv(\"../Data/train_data.csv\")\n",
    "\n",
    "#load in morph embedding df\n",
    "morph_embeddings = pd.read_csv(\"../Data/morph_embeddings.csv\")\n",
    "\n",
    "# merge ADP data with morph embedding data\n",
    "full_data = (\n",
    "    data.merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# get only the morph embeddings back out (some pre-morph embeddings\n",
    "# will be null)\n",
    "morph_embed = full_data.filter(regex=\"_morph_emb_\")\n",
    "\n",
    "# complete the imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_morph_embed = imputer.fit_transform(morph_embed)\n",
    "imputed_morph_embed_df = pd.DataFrame(imputed_morph_embed)\n",
    "csv_imputed_morph_embed_df = imputed_morph_embed_df.copy()\n",
    "\n",
    "# save to CSV\n",
    "csv_imputed_morph_embed_df.to_csv('../Data/imputed_morph_embed.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_morph_emb_0</th>\n",
       "      <th>pre_morph_emb_1</th>\n",
       "      <th>pre_morph_emb_2</th>\n",
       "      <th>pre_morph_emb_3</th>\n",
       "      <th>pre_morph_emb_4</th>\n",
       "      <th>pre_morph_emb_5</th>\n",
       "      <th>pre_morph_emb_6</th>\n",
       "      <th>pre_morph_emb_7</th>\n",
       "      <th>pre_morph_emb_8</th>\n",
       "      <th>pre_morph_emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>post_morph_emb_22</th>\n",
       "      <th>post_morph_emb_23</th>\n",
       "      <th>post_morph_emb_24</th>\n",
       "      <th>post_morph_emb_25</th>\n",
       "      <th>post_morph_emb_26</th>\n",
       "      <th>post_morph_emb_27</th>\n",
       "      <th>post_morph_emb_28</th>\n",
       "      <th>post_morph_emb_29</th>\n",
       "      <th>post_morph_emb_30</th>\n",
       "      <th>post_morph_emb_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pre_morph_emb_0  pre_morph_emb_1  pre_morph_emb_2  pre_morph_emb_3  \\\n",
       "0         0.373316         0.209818        -0.546946         0.630883   \n",
       "1         0.373316         0.209818        -0.546946         0.630883   \n",
       "2         0.373316         0.209818        -0.546946         0.630883   \n",
       "3         0.373316         0.209818        -0.546946         0.630883   \n",
       "4         0.373316         0.209818        -0.546946         0.630883   \n",
       "\n",
       "   pre_morph_emb_4  pre_morph_emb_5  pre_morph_emb_6  pre_morph_emb_7  \\\n",
       "0         0.832248        -0.983688         1.085743        -0.395466   \n",
       "1         0.832248        -0.983688         1.085743        -0.395466   \n",
       "2         0.832248        -0.983688         1.085743        -0.395466   \n",
       "3         0.832248        -0.983688         1.085743        -0.395466   \n",
       "4         0.832248        -0.983688         1.085743        -0.395466   \n",
       "\n",
       "   pre_morph_emb_8  pre_morph_emb_9  ...  post_morph_emb_22  \\\n",
       "0        -1.151271        -0.495176  ...          -1.064851   \n",
       "1        -1.151271        -0.495176  ...          -1.064851   \n",
       "2        -1.151271        -0.495176  ...          -1.064851   \n",
       "3        -1.151271        -0.495176  ...          -1.064851   \n",
       "4        -1.151271        -0.495176  ...          -1.064851   \n",
       "\n",
       "   post_morph_emb_23  post_morph_emb_24  post_morph_emb_25  post_morph_emb_26  \\\n",
       "0          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "1          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "2          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "3          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "4          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "\n",
       "   post_morph_emb_27  post_morph_emb_28  post_morph_emb_29  post_morph_emb_30  \\\n",
       "0          -0.525224           0.171648           1.022962          -0.645146   \n",
       "1          -0.525224           0.171648           1.022962          -0.645146   \n",
       "2          -0.525224           0.171648           1.022962          -0.645146   \n",
       "3          -0.525224           0.171648           1.022962          -0.645146   \n",
       "4          -0.525224           0.171648           1.022962          -0.645146   \n",
       "\n",
       "   post_morph_emb_31  \n",
       "0          -0.687774  \n",
       "1          -0.687774  \n",
       "2          -0.687774  \n",
       "3          -0.687774  \n",
       "4          -0.687774  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the resulting dataframe\n",
    "pd.read_csv('Data/imputed_morph_embed.csv').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(train, feature, imp_morph):\n",
    "    \"\"\"\n",
    "    Function that performs data cleaning and feature engineering for the data\n",
    "\n",
    "    inputs:\n",
    "    - train: path to training data\n",
    "    - feature: path to feature data\n",
    "    - imp_morph: path to imputed morph embedding data\n",
    "\n",
    "    outputs:\n",
    "    - data: cleaned and feature engineered data\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(train)\n",
    "\n",
    "    ############## CONCAT FEATURE WEIGHT DATA ##############\n",
    "    \n",
    "    #load in feature weight information for each neuron\n",
    "    feature_weights = pd.read_csv(feature)\n",
    "        \n",
    "    # make feature_weights into a numPy array\n",
    "    feature_weights[\"feature_weights\"] = (feature_weights.filter(regex=\"feature_weight_\").sort_index(axis=1)\n",
    "                                        .apply(lambda x: np.array(x), axis=1))\n",
    "\n",
    "    # delete the feature_weight_i columns\n",
    "    feature_weights.drop(\n",
    "        feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True)\n",
    "    \n",
    "    # update data with feature weight information for pre- and post- neurons\n",
    "    data = (\n",
    "    data.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "        how=\"left\", \n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    ))\n",
    "    \n",
    "    ############## CONCAT IMPUTED MORPH EMBEDDINGS ################\n",
    "    # load in imputed morph embedding data\n",
    "    morph_embs = pd.read_csv(imp_morph)\n",
    "\n",
    "    # put all the morph embedding data into vectors\n",
    "    morph_embs[\"pre_morph_embeddings\"] = (morph_embs.filter(regex=\"pre_morph_emb_\").sort_index(axis=1)\n",
    "                                          .apply(lambda x: np.array(x), axis=1))\n",
    "    \n",
    "    morph_embs[\"post_morph_embeddings\"] = (morph_embs.filter(regex=\"post_morph_emb_\")\n",
    "                                           .sort_index(axis=1).apply(lambda x: np.array(x), axis=1))\n",
    "    \n",
    "    # drop the individual morph embedding columns\n",
    "    morph_embs.drop(morph_embs.filter(regex=\"_morph_emb_\").columns, axis=1, inplace=True)\n",
    "    morph_embs[\"ID\"] = data[\"ID\"]\n",
    "\n",
    "    # merge the main df with morph embeddings\n",
    "    data = data.merge(morph_embs, on=\"ID\")\n",
    "\n",
    "    ############## FE: CALCULATE SIMILARITY BETWEEN PRE- AND POST- MORPH EMBEDDINGS ###################\n",
    "    data[\"me_similarity\"] = data.apply(row_morph_similarity, axis=1)\n",
    "    \n",
    "    ############## FE: CALCULATE SIMILARITY BETWEEN PRE- AND POST- FEATURE WEIGHTS ##############\n",
    "    data[\"fw_similarity\"] = data.apply(row_feature_similarity, axis=1)\n",
    "    \n",
    "    ############## FE: COMBINE COORDINATES INTO ARRAYS ##############\n",
    "    data = coord_column(data, \"axonal_coords\", \"axonal_coor_\")\n",
    "    data = coord_column(data, \"dendritic_coords\", \"dendritic_coor_\")\n",
    "    data = coord_column(data, \"pre_rf_coords\", \"pre_rf_[xy]\")\n",
    "    data = coord_column(data, \"post_rf_coords\", \"post_rf_[xy]\")\n",
    "    data = coord_column(data, \"pre_nucleus_coords\", \"pre_nucleus_[xyz]\")\n",
    "    data = coord_column(data, \"post_nucleus_coords\", \"post_nucleus_[xyz]\")\n",
    "    data = coord_column(data, \"pre_nucleus_xy\", \"pre_nucleus_[xy]\")\n",
    "    data = coord_column(data, \"post_nucleus_xy\", \"post_nucleus_[xy]\")\n",
    "\n",
    "    ############## FE: RF Distance ##############\n",
    "    data = coord_rf(data)\n",
    "    data[\"rf_distance\"] = data.apply(rfdistance, axis=1)\n",
    "\n",
    "    ############## FE: ONE HOT ENCODE BRAIN AREA ##############\n",
    "    data = one_hot('pre_brain_area', data, '_pre')\n",
    "    data = one_hot('post_brain_area', data, '_post')\n",
    "\n",
    "    ############## FE: BRAIN COMPARTMENT GROUPING ##############\n",
    "    data = area_cols(data)\n",
    "\n",
    "    ############## FE: MINICOLUMNS ##############\n",
    "    data[\"minicol_dist\"] =  data[[\"pre_nucleus_xy\", \"post_nucleus_xy\"]].apply(\n",
    "    lambda x: math.dist(x[\"pre_nucleus_xy\"], x[\"post_nucleus_xy\"]), axis=1)\n",
    "\n",
    "    ############## FE: DISTANCE FROM PRE-SYNAPTIC NUCLEUS TO AXON ##############\n",
    "    data[\"nuclei_adp_dist\"] =  data[[\"pre_nucleus_coords\", \"axonal_coords\"]].apply(\n",
    "    lambda x: math.dist(x[\"pre_nucleus_coords\"], x[\"axonal_coords\"]), axis=1)\n",
    "\n",
    "    ############## STANDARDIZE ALL NUMERIC DATA #############\n",
    "    num_cols = data.select_dtypes(include='number').drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id'])\n",
    "    num_cols = num_cols.columns\n",
    "    for column in num_cols:\n",
    "        data[column] = StandardScaler().fit_transform(np.array(data[column]).reshape(-1, 1))\n",
    "    \n",
    "    # return processed data\n",
    "    return data\n",
    "\n",
    "def row_feature_similarity(row):\n",
    "    \"\"\"\n",
    "    Cosine similarity function for feature weight similarity\n",
    "\n",
    "    Inputs: row - a row of the dataframe containing feature weight information\n",
    "    Outputs: the cosine similarity between the pre and post feature weights\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_feature_weights\"]\n",
    "    post = row[\"post_feature_weights\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "def row_morph_similarity(row):\n",
    "    \"\"\"\n",
    "    Morph embedding similarity function for feature weight similarity\n",
    "\n",
    "    Inputs: row - a row of the dataframe containing feature weight information\n",
    "    Outputs: the cosine similarity between the pre and post morph embeddings\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_morph_embeddings\"]\n",
    "    post = row[\"post_morph_embeddings\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "\n",
    "def coord_column(df, new_col, old_cols):\n",
    "    \"\"\"\n",
    "    Function that combines coordinate data into an array of coordinates\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "        - new_col: the new column created to store the coordinate array\n",
    "        - old_cols: the old coordinate columns to be combined to form the new column\n",
    "    Outputs:\n",
    "        - df: the updated data frame\n",
    "    \"\"\"\n",
    "    df[new_col] = (\n",
    "        df.filter(regex=old_cols)\n",
    "        .sort_index(axis=1)\n",
    "        .apply(lambda x: np.array(x), axis=1)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coord_rf(df):\n",
    "    \"\"\"\n",
    "    Function that combines coordinate data for the rf data \n",
    "    (readout location of deep learning model)\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "    Outputs:\n",
    "        - df: the updated data frame\n",
    "    \"\"\"\n",
    "    df = coord_column(df, \"pre_rf_coords_xy\", \"pre_rf_[xy]\")\n",
    "    df = coord_column(df, \"post_rf_coords_xy\", \"post_rf_[xy]\")\n",
    "    return df\n",
    "\n",
    "def rfdistance(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the distance between the rf locations\n",
    "    Inputs:\n",
    "        - row: a row describing one ADP\n",
    "    Outputs:\n",
    "        - the distance between the pre- and post- neuron readout locations\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_rf_coords_xy\"]\n",
    "    post = row[\"post_rf_coords_xy\"]\n",
    "    return math.dist(pre, post)\n",
    "\n",
    "def one_hot(column, df, suffix=''):\n",
    "    \"\"\"\n",
    "    Function for one-hot encoding\n",
    "\n",
    "    Inputs:\n",
    "        - column: the column to be one-hot encoded\n",
    "        - df: the dataframe\n",
    "        - suffix: an optional suffix to be added to the column when it is one-hot encoded\n",
    "    \n",
    "    Outputs:\n",
    "        - df: the updated dataframe\n",
    "    \"\"\"\n",
    "    cats = pd.unique(df[column])\n",
    "\n",
    "    for cat in cats:\n",
    "        new_col = cat+suffix\n",
    "        df[new_col] = df[column]==cat\n",
    "        df[new_col] = df[new_col].astype('int')\n",
    "    \n",
    "    df = df.drop(columns=column)\n",
    "    return df\n",
    "\n",
    "def area_cols(df):\n",
    "    \"\"\"\n",
    "    Function that groups and encodes the compartment data\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "    Outputs:\n",
    "        - df: the updated data frame with grouped compartments\n",
    "    \"\"\"\n",
    "    # Encode brain areas\n",
    "    area1 = [\"basal\", \"soma\"] # the cell body\n",
    "    area2 = [\"axon\", \"apical\", \"oblique\", \"apical_shaft\"] # axonal areas\n",
    "    area3 = [\"apical_tuft\"] # terminal areas\n",
    "    df[\"area1\"] = df[\"compartment\"].isin(area1).astype('int')\n",
    "    df[\"area2\"] = df[\"compartment\"].isin(area2).astype('int')\n",
    "    df[\"area3\"] = df[\"compartment\"].isin(area3).astype('int')\n",
    "    return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
