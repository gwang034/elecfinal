{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "sys.path.insert(0, '/Users/gracewang/Documents/GitHub/elecfinal')\n",
    "sys.path.insert(0, 'D:\\Fall23 Coursework\\ELEC478\\Competition\\elecfinal')\n",
    "from ml_pipeline import train_n_predict, validation, clean_split\n",
    "from Data.data_cleaner import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "feature_path = \"../Data/feature_weights.csv\"\n",
    "morph_path = \"../Data/imputed_morph_embed.csv\"\n",
    "X_train, X_val, X_query, y_train, y_val, y_query = clean_split(train_path, feature_path, morph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample X_train_feat\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "X_train, y_train = ros.fit_resample(\n",
    "        X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = X_train.copy()\n",
    "X_val_feat = X_val.copy()\n",
    "X_query_feat = X_query.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(column, df, suffix=''):\n",
    "    \"\"\"\n",
    "    one-hot encodes this shit\n",
    "    \"\"\"\n",
    "    cats = pd.unique(df[column])\n",
    "\n",
    "    for cat in cats:\n",
    "        new_col = cat+suffix\n",
    "        df[new_col] = df[column]==cat\n",
    "        df[new_col] = df[new_col].astype('int')\n",
    "    \n",
    "    df = df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode brain areas for all\n",
    "X_train_feat = one_hot('pre_brain_area', X_train_feat, '_pre')\n",
    "X_train_feat = one_hot('post_brain_area', X_train_feat, '_post')\n",
    "\n",
    "X_val_feat = one_hot('pre_brain_area', X_val_feat, '_pre')\n",
    "X_val_feat = one_hot('post_brain_area', X_val_feat, '_post')\n",
    "\n",
    "X_query_feat = one_hot('pre_brain_area', X_query_feat, '_pre')\n",
    "X_query_feat = one_hot('post_brain_area', X_query_feat, '_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode brain areas\n",
    "area1 = [\"basal\", \"soma\"]\n",
    "area2 = [\"axon\", \"apical\", \"oblique\", \"apical_shaft\"]\n",
    "area3 = [\"apical_tuft\"]\n",
    "\n",
    "def area_cols(df):\n",
    "    df[\"area1\"] = df[\"compartment\"].isin(area1).astype('int')\n",
    "    df[\"area2\"] = df[\"compartment\"].isin(area2).astype('int')\n",
    "    df[\"area3\"] = df[\"compartment\"].isin(area3).astype('int')\n",
    "    df.drop(columns='compartment')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = area_cols(X_train_feat)\n",
    "X_val_feat = area_cols(X_val_feat)\n",
    "X_query_feat = area_cols(X_query_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = X_train_feat.select_dtypes('number')\n",
    "X_val_feat = X_val_feat.select_dtypes('number')\n",
    "X_query_feat = X_query_feat.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                  int64\n",
       "axonal_coor_x                     float64\n",
       "axonal_coor_y                     float64\n",
       "axonal_coor_z                     float64\n",
       "dendritic_coor_x                  float64\n",
       "dendritic_coor_y                  float64\n",
       "dendritic_coor_z                  float64\n",
       "adp_dist                          float64\n",
       "post_skeletal_distance_to_soma    float64\n",
       "pre_skeletal_distance_to_soma     float64\n",
       "pre_oracle                        float64\n",
       "pre_test_score                    float64\n",
       "pre_rf_x                          float64\n",
       "pre_rf_y                          float64\n",
       "post_oracle                       float64\n",
       "post_test_score                   float64\n",
       "post_rf_x                         float64\n",
       "post_rf_y                         float64\n",
       "pre_nucleus_x                     float64\n",
       "pre_nucleus_y                     float64\n",
       "pre_nucleus_z                     float64\n",
       "post_nucleus_x                    float64\n",
       "post_nucleus_y                    float64\n",
       "post_nucleus_z                    float64\n",
       "pre_nucleus_id                      int64\n",
       "post_nucleus_id                     int64\n",
       "me_similarity                     float64\n",
       "fw_similarity                     float64\n",
       "nuclei_adp_dist                   float64\n",
       "RL_pre                              int32\n",
       "AL_pre                              int32\n",
       "V1_pre                              int32\n",
       "RL_post                             int32\n",
       "V1_post                             int32\n",
       "AL_post                             int32\n",
       "area1                               int32\n",
       "area2                               int32\n",
       "area3                               int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_feat.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_path = \"../Data/leaderboard_data.csv\"\n",
    "sub_data = cleaner(leaderboard_path, feature_path, morph_path, submission = True)\n",
    "sub_data = area_cols(sub_data)\n",
    "sub_data = one_hot('pre_brain_area', sub_data, '_pre')\n",
    "sub_data = one_hot('post_brain_area', sub_data, '_post')\n",
    "sub_data = sub_data.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42593 entries, 0 to 42592\n",
      "Data columns (total 38 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   ID                              42593 non-null  int64  \n",
      " 1   axonal_coor_x                   42593 non-null  float64\n",
      " 2   axonal_coor_y                   42593 non-null  float64\n",
      " 3   axonal_coor_z                   42593 non-null  float64\n",
      " 4   dendritic_coor_x                42593 non-null  float64\n",
      " 5   dendritic_coor_y                42593 non-null  float64\n",
      " 6   dendritic_coor_z                42593 non-null  float64\n",
      " 7   adp_dist                        42593 non-null  float64\n",
      " 8   post_skeletal_distance_to_soma  42593 non-null  float64\n",
      " 9   pre_skeletal_distance_to_soma   42593 non-null  float64\n",
      " 10  pre_oracle                      42593 non-null  float64\n",
      " 11  pre_test_score                  42593 non-null  float64\n",
      " 12  pre_rf_x                        42593 non-null  float64\n",
      " 13  pre_rf_y                        42593 non-null  float64\n",
      " 14  post_oracle                     42593 non-null  float64\n",
      " 15  post_test_score                 42593 non-null  float64\n",
      " 16  post_rf_x                       42593 non-null  float64\n",
      " 17  post_rf_y                       42593 non-null  float64\n",
      " 18  pre_nucleus_x                   42593 non-null  float64\n",
      " 19  pre_nucleus_y                   42593 non-null  float64\n",
      " 20  pre_nucleus_z                   42593 non-null  float64\n",
      " 21  post_nucleus_x                  42593 non-null  float64\n",
      " 22  post_nucleus_y                  42593 non-null  float64\n",
      " 23  post_nucleus_z                  42593 non-null  float64\n",
      " 24  pre_nucleus_id                  42593 non-null  int64  \n",
      " 25  post_nucleus_id                 42593 non-null  int64  \n",
      " 26  me_similarity                   42593 non-null  float64\n",
      " 27  fw_similarity                   42593 non-null  float64\n",
      " 28  nuclei_adp_dist                 42593 non-null  float64\n",
      " 29  area1                           42593 non-null  int32  \n",
      " 30  area2                           42593 non-null  int32  \n",
      " 31  area3                           42593 non-null  int32  \n",
      " 32  AL_pre                          42593 non-null  int32  \n",
      " 33  RL_pre                          42593 non-null  int32  \n",
      " 34  V1_pre                          42593 non-null  int32  \n",
      " 35  V1_post                         42593 non-null  int32  \n",
      " 36  RL_post                         42593 non-null  int32  \n",
      " 37  AL_post                         42593 non-null  int32  \n",
      "dtypes: float64(26), int32(9), int64(3)\n",
      "memory usage: 10.9 MB\n"
     ]
    }
   ],
   "source": [
    "sub_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638329736854478\n",
      "0.5872868660699375\n",
      "0.6964426473759215\n",
      "0.6941604910450108\n",
      "0.6725076077281873\n",
      "0.7026727161008225\n",
      "0.6891227726014783\n",
      "0.7188520071918769\n",
      "0.7261785291022962\n",
      "0.6934668380871164\n",
      "0.7022237101719014\n",
      "0.7103940941007478\n",
      "0.7175116365410898\n",
      "0.7045548283336602\n",
      "0.7139968417997851\n",
      "0.7148889360853556\n",
      "0.7282940206580016\n",
      "0.7169210952534586\n",
      "0.715123044971928\n",
      "0.7159866862730158\n",
      "0.7170136687813206\n",
      "0.7335968949768727\n",
      "0.7122616150594918\n",
      "0.7254312937434237\n",
      "0.7376538366133977\n",
      "0.741868364010136\n",
      "0.7416752728162943\n",
      "0.7294446236829348\n",
      "0.7424555817297793\n",
      "0.7376155750502184\n",
      "0.7373424750367619\n",
      "0.7377286574244456\n",
      "0.7368668805639366\n",
      "0.741416034513227\n",
      "0.7417632257740265\n",
      "0.7246830045703113\n",
      "0.742271731676197\n",
      "0.7357844512140751\n",
      "0.7414285992214744\n",
      "0.7414285992214744\n",
      "0.7416124492750569\n",
      "0.7414537286379694\n",
      "0.7424614993020507\n",
      "0.7357168449774403\n",
      "0.7363140334010476\n",
      "0.7248966046105184\n",
      "0.741416034513227\n",
      "0.7246830045703113\n",
      "0.741390905096732\n",
      "0.742033002219495\n",
      "0.6364324659091093\n",
      "0.6168150653283766\n",
      "0.6279143232810263\n",
      "0.6079378962949513\n",
      "0.635948521984997\n",
      "0.6597387027060329\n",
      "0.717143936433925\n",
      "0.6332351124419795\n",
      "0.6663250417067251\n",
      "0.6638372294737251\n",
      "0.6167119536581135\n",
      "0.7139306946905597\n",
      "0.7156348744420864\n",
      "0.7157605215245612\n",
      "0.7134724476213792\n",
      "0.7122741797677394\n",
      "0.7147255948781386\n",
      "0.7143142830739599\n",
      "0.7164641451864198\n",
      "0.7226419285124845\n",
      "0.724482293488887\n",
      "0.7323978975595283\n",
      "0.7126808710017882\n",
      "0.7392033488595298\n",
      "0.7308439268685343\n",
      "0.738959998832698\n",
      "0.7318914182232044\n",
      "0.7444308349289162\n",
      "0.7403593020182975\n",
      "0.7426189229369965\n",
      "0.7326122271634401\n",
      "0.7387589635007386\n",
      "0.7370534867470699\n",
      "0.7416878375245417\n",
      "0.7250348164012406\n",
      "0.741416034513227\n",
      "0.7357216276728378\n",
      "0.7370534867470699\n",
      "0.7362935245546824\n",
      "0.7366340686795059\n",
      "0.742033002219495\n",
      "0.741416034513227\n",
      "0.736968695232058\n",
      "0.7420204375112475\n",
      "0.7422340375514545\n",
      "0.7360766009464873\n",
      "0.7363153304031893\n",
      "0.7235224308414139\n",
      "0.741416034513227\n",
      "0.725072510525983\n",
      "0.5992107741967909\n",
      "0.5415394929045877\n",
      "0.5767173524295282\n",
      "0.6152300476486161\n",
      "0.7100687086884552\n",
      "0.6321532505305549\n",
      "0.6333058801213345\n",
      "0.6496776949677938\n",
      "0.6811606385790046\n",
      "0.6529013127282925\n",
      "0.6948310411522567\n",
      "0.7150317684462076\n",
      "0.7191371044751438\n",
      "0.7187270896731068\n",
      "0.710725397085312\n",
      "0.7129175738926439\n",
      "0.7154338391101268\n",
      "0.7265647114899798\n",
      "0.7331637773241872\n",
      "0.7351985304965735\n",
      "0.7335261272975178\n",
      "0.7389692399729575\n",
      "0.7290174236025208\n",
      "0.7169508452400832\n",
      "0.7411944903349021\n",
      "0.7318662888067093\n",
      "0.7379725748897144\n",
      "0.7425607199658888\n",
      "0.7399962224812624\n",
      "0.7375653162172284\n",
      "0.7385340957544256\n",
      "0.7370660514553176\n",
      "0.7376532691749608\n",
      "0.7405398285038918\n",
      "0.7390525723605601\n",
      "0.7414034698049795\n",
      "0.7423345552174343\n",
      "0.742007872803\n",
      "0.7364786716104065\n",
      "0.7235349955496614\n",
      "0.7235224308414139\n",
      "0.7414788580544643\n",
      "0.7468696853310679\n",
      "0.7421686200059339\n",
      "0.7235098661331665\n",
      "0.7357844512140751\n",
      "0.742221472843207\n",
      "0.7235098661331665\n",
      "0.7235349955496614\n",
      "0.7235098661331665\n",
      "0.5235812012509586\n",
      "0.6217722075138576\n",
      "0.6902979376044694\n",
      "0.6200231190631753\n",
      "0.6205111161189805\n",
      "0.5514114625806776\n",
      "0.700817759850326\n",
      "0.711024923517405\n",
      "0.721995210819592\n",
      "0.643262317061901\n",
      "0.6813816153188923\n",
      "0.7136575946771032\n",
      "0.6992617625934855\n",
      "0.7126147238925629\n",
      "0.7153663949987598\n",
      "0.7101461235037865\n",
      "0.7146627713369011\n",
      "0.7128819063337478\n",
      "0.7136609182450913\n",
      "0.7371698926892853\n",
      "0.7172331863937991\n",
      "0.7127562592512731\n",
      "0.7183203984390579\n",
      "0.7346066922068005\n",
      "0.7378417397986728\n",
      "0.7110005236646146\n",
      "0.7378417397986728\n",
      "0.7410013991410603\n",
      "0.730160812053041\n",
      "0.7378040456739304\n",
      "0.7416752728162943\n",
      "0.7418134846070165\n",
      "0.7369027102481003\n",
      "0.7370534867470699\n",
      "0.7415496257338194\n",
      "0.741390905096732\n",
      "0.7470066001196485\n",
      "0.728891776520046\n",
      "0.7414788580544643\n",
      "0.7420706963442374\n",
      "0.7235349955496614\n",
      "0.7424112404690608\n",
      "0.7424614993020507\n",
      "0.7414285992214744\n",
      "0.7423345552174343\n",
      "0.7421083904689798\n",
      "0.7415291168874543\n",
      "0.742033002219495\n",
      "0.7366591980960009\n",
      "0.7235098661331665\n",
      "0.5390232276871048\n",
      "0.6090482111908587\n",
      "0.6229479399552859\n",
      "0.7099561937526649\n",
      "0.6101618496547543\n",
      "0.6122237588094817\n",
      "0.653487963009499\n",
      "0.7165500715783057\n",
      "0.717376018754651\n",
      "0.6690840895126029\n",
      "0.6633908175490875\n",
      "0.7164270185001143\n",
      "0.7271532262117648\n",
      "0.713045247540965\n",
      "0.7192871514104088\n",
      "0.7163338775338153\n",
      "0.7374224838563765\n",
      "0.7132085887481822\n",
      "0.7132965417059145\n",
      "0.7324799329449894\n",
      "0.7123746974337192\n",
      "0.7393495047883698\n",
      "0.7288124972641361\n",
      "0.7302884857013621\n",
      "0.731803465265472\n",
      "0.7348677949504464\n",
      "0.7287786941458188\n",
      "0.7317360211541049\n",
      "0.7260800380021628\n",
      "0.73910283119355\n",
      "0.7418386140235114\n",
      "0.7412652580142574\n",
      "0.7417380963575316\n",
      "0.7414034698049795\n",
      "0.7372543599537619\n",
      "0.741416034513227\n",
      "0.7282919940921553\n",
      "0.7391610341646577\n",
      "0.737028357330575\n",
      "0.7420270846472236\n",
      "0.7415291168874543\n",
      "0.7422466022597021\n",
      "0.738184148364075\n",
      "0.7415039874709592\n",
      "0.725072510525983\n",
      "0.7372543599537619\n",
      "0.7424112404690608\n",
      "0.7424112404690608\n",
      "0.7235726896744038\n",
      "0.7250348164012406\n",
      "0.6517952130893456\n",
      "0.580321964569144\n",
      "0.6676188013430457\n",
      "0.6030620599312264\n",
      "0.5833125272573106\n",
      "0.6053026311309697\n",
      "0.5950570437754436\n",
      "0.658334049386599\n",
      "0.7239327698939862\n",
      "0.7154417832482446\n",
      "0.6433914498376315\n",
      "0.7141938240000518\n",
      "0.7150476567224432\n",
      "0.7096963880111606\n",
      "0.7112392531213168\n",
      "0.6655816973542777\n",
      "0.7201971605380614\n",
      "0.7127390739728959\n",
      "0.718747598519472\n",
      "0.70862911737383\n",
      "0.7334209890614082\n",
      "0.7356039247284807\n",
      "0.7158312892039163\n",
      "0.7142937742275948\n",
      "0.7370553511876486\n",
      "0.7416673286781765\n",
      "0.7295120677943019\n",
      "0.7408162520853363\n",
      "0.7399162136616478\n",
      "0.7392536076925197\n",
      "0.7470317295361435\n",
      "0.7389348694162031\n",
      "0.7425309699792642\n",
      "0.7362744748357266\n",
      "0.7334335537696557\n",
      "0.7390274429440651\n",
      "0.7414034698049795\n",
      "0.7414788580544643\n",
      "0.7357844512140751\n",
      "0.7298969531798439\n",
      "0.7399321019378833\n",
      "0.7250348164012406\n",
      "0.7235098661331665\n",
      "0.725072510525983\n",
      "0.7250348164012406\n",
      "0.741800919898769\n",
      "0.725072510525983\n",
      "0.7415291168874543\n",
      "0.7235224308414139\n",
      "0.7248840399022709\n",
      "0.5922028284374204\n",
      "0.6171860890035294\n",
      "0.6472937239687617\n",
      "0.6925304025408272\n",
      "0.6989252716003548\n",
      "0.6341702509861269\n",
      "0.7342681746478233\n",
      "0.7256495143537606\n",
      "0.7063284787624006\n",
      "0.7250040936630097\n",
      "0.7398892198045742\n",
      "0.7308023417373668\n",
      "0.721798796057762\n",
      "0.7416521699656456\n",
      "0.7162333598678354\n",
      "0.7352859160158688\n",
      "0.7409439257336574\n",
      "0.7231088492834874\n",
      "0.736853180978815\n",
      "0.734323054050943\n",
      "0.741793705324356\n",
      "0.7420760464780718\n",
      "0.7425627465317353\n",
      "0.7417586052038968\n",
      "0.7417037258007773\n",
      "0.735574174741856\n",
      "0.7432743143317115\n",
      "0.749036165283468\n",
      "0.749723900669091\n",
      "0.7474312061957793\n",
      "0.7418637434400064\n",
      "0.7502390537072374\n",
      "0.7461529295225249\n",
      "0.7432207319307336\n",
      "0.7466773237009308\n",
      "0.7439838555658416\n",
      "0.7450564763370066\n",
      "0.7509347332309783\n",
      "0.7415244963173245\n",
      "0.7481943298308871\n",
      "0.7455272881144346\n",
      "0.7467037501195674\n",
      "0.7440605408174681\n",
      "0.7429614936276663\n",
      "0.743108946558648\n",
      "0.7460457647205689\n",
      "0.7457865264175016\n",
      "0.7427194406029762\n",
      "0.744563129147367\n",
      "0.7464445118165002\n",
      "0.6564294017415496\n",
      "0.667946051195917\n",
      "0.6056319075496874\n",
      "0.6432704233252866\n",
      "0.6456510707563305\n",
      "0.6708300165205647\n",
      "0.6686138451736119\n",
      "0.726813411650646\n",
      "0.7283291207784607\n",
      "0.7350312982829312\n",
      "0.7351305189467694\n",
      "0.7376824517231484\n",
      "0.7254941172846612\n",
      "0.7405815757603269\n",
      "0.7298804974651714\n",
      "0.7375673427830747\n",
      "0.7360239102344818\n",
      "0.7404731139562295\n",
      "0.7394394843119485\n",
      "0.7349102717705862\n",
      "0.7396630550561196\n",
      "0.7415629200057716\n",
      "0.7388628047347063\n",
      "0.7413493199655645\n",
      "0.7525727658732797\n",
      "0.7386009724273557\n",
      "0.750493671440175\n",
      "0.7419001405626071\n",
      "0.7444395086307387\n",
      "0.7437266438286206\n",
      "0.7436181820245231\n",
      "0.7447582469070552\n",
      "0.7452336792546128\n",
      "0.7475125120175354\n",
      "0.7420727229100836\n",
      "0.7460239588720621\n",
      "0.7431671495297556\n",
      "0.7451695587112338\n",
      "0.7457018970277575\n",
      "0.7453170116422154\n",
      "0.7410331756935313\n",
      "0.7498369830433183\n",
      "0.7448005616019273\n",
      "0.7445082497442475\n",
      "0.7457283234463941\n",
      "0.743942837873111\n",
      "0.7464901500793604\n",
      "0.7465701588989748\n",
      "0.746694508979308\n",
      "0.7440559202473384\n",
      "0.6396272253719559\n",
      "0.6384091782356556\n",
      "0.6370647544531758\n",
      "0.6480389327617877\n",
      "0.7056010226861886\n",
      "0.6784414735889832\n",
      "0.7237211964196255\n",
      "0.7234619581165583\n",
      "0.669964186528363\n",
      "0.7116069532284814\n",
      "0.7117919381589379\n",
      "0.7097348116996078\n",
      "0.7244578936360968\n",
      "0.7352607865993739\n",
      "0.7265257203630957\n",
      "0.7350174365725421\n",
      "0.7376539987386654\n",
      "0.7395102519913036\n",
      "0.7433060908841825\n",
      "0.7366990809118574\n",
      "0.7408216022191707\n",
      "0.7463301324401312\n",
      "0.7422268229770415\n",
      "0.7353738689736011\n",
      "0.7506378008031687\n",
      "0.7391802460088812\n",
      "0.7437107555523852\n",
      "0.7419550199657268\n",
      "0.7448746528492705\n",
      "0.747486085598899\n",
      "0.7458731823730924\n",
      "0.7386492046944992\n",
      "0.7426553200595973\n",
      "0.7459406264844595\n",
      "0.745625211776131\n",
      "0.7442523350091681\n",
      "0.7451900675575991\n",
      "0.744922885116414\n",
      "0.747581982694749\n",
      "0.7446451645328279\n",
      "0.7444864438957405\n",
      "0.7455088058339157\n",
      "0.7425309699792642\n",
      "0.7466032324535876\n",
      "0.7441438732050707\n",
      "0.7434276848349646\n",
      "0.7458665352371163\n",
      "0.7443072144122878\n",
      "0.7447846733256919\n",
      "0.7444969820381415\n",
      "0.6981726861076479\n",
      "0.616314503564324\n",
      "0.6025919777175032\n",
      "0.5968062943513935\n",
      "0.6942604412725537\n",
      "0.6230696960313356\n",
      "0.6802469005701945\n",
      "0.6744863466205799\n",
      "0.71331242998215\n",
      "0.704228713357663\n",
      "0.7149074183658746\n",
      "0.7304015680755893\n",
      "0.7215620931669063\n",
      "0.7328265567673519\n",
      "0.7370859928632457\n",
      "0.7448025881677738\n",
      "0.7265369880692016\n",
      "0.7409677581480106\n",
      "0.7442874351296274\n",
      "0.7329396391415792\n",
      "0.7348369100869478\n",
      "0.742955576055395\n",
      "0.7480938121649073\n",
      "0.7396166872295549\n",
      "0.7452357058204593\n",
      "0.7388456194563291\n",
      "0.7441584644791644\n",
      "0.7445697762833431\n",
      "0.7442338527286493\n",
      "0.7419093817028666\n",
      "0.7429945671822791\n",
      "0.7473353090999292\n",
      "0.7471375973359576\n",
      "0.7435804878997806\n",
      "0.7448680057132944\n",
      "0.7451067351699965\n",
      "0.7470165708236125\n",
      "0.7492795153102996\n",
      "0.7445823409915906\n",
      "0.7472453295763505\n",
      "0.7453547057669578\n",
      "0.7463142441638957\n",
      "0.7443197791205354\n",
      "0.7461303941103132\n",
      "0.7453262527824749\n",
      "0.747474817892793\n",
      "0.7432643436277475\n",
      "0.7455180469741751\n",
      "0.7448144233123165\n",
      "0.7434104995565873\n",
      "0.6137459529480047\n",
      "0.6177594450127836\n",
      "0.6847827602475329\n",
      "0.666495192175186\n",
      "0.7106315265553083\n",
      "0.6337330802017487\n",
      "0.7386102135676151\n",
      "0.7315344183837083\n",
      "0.7148102242678829\n",
      "0.7252587113959472\n",
      "0.7137765946236019\n",
      "0.7274634529115266\n",
      "0.7317836859828115\n",
      "0.7370555133129164\n",
      "0.7302825681290906\n",
      "0.744655135236792\n",
      "0.7392669019644719\n",
      "0.7429740583359138\n",
      "0.7474470944720147\n",
      "0.7374496398387178\n",
      "0.7371765398252614\n",
      "0.743825134928754\n",
      "0.7384184193759149\n",
      "0.7437940879399877\n",
      "0.743258426055476\n",
      "0.7413347286914708\n",
      "0.7444600174771039\n",
      "0.7498323624731885\n",
      "0.7414557552038158\n",
      "0.7406562344461072\n",
      "0.7467447678122978\n",
      "0.7479350915278198\n",
      "0.7453500851968282\n",
      "0.7419801493822217\n",
      "0.7440777260958452\n",
      "0.7454300940164427\n",
      "0.7458698588051044\n",
      "0.7451662351432458\n",
      "0.742424534741013\n",
      "0.7428107171286966\n",
      "0.7473696796566835\n",
      "0.7458790999453637\n",
      "0.7445836379937323\n",
      "0.7440559202473384\n",
      "0.7461337176783014\n",
      "0.7439051437483686\n",
      "0.7458401088184797\n",
      "0.7458401088184797\n",
      "0.7446510821050993\n",
      "0.7448508204349172\n",
      "0.6071291343969831\n",
      "0.6509725894809884\n",
      "0.6610083056774647\n",
      "0.6927910999713038\n",
      "0.6904586848074032\n",
      "0.7000017833779448\n",
      "0.7312295418177809\n",
      "0.7422870525139955\n",
      "0.7336750393559087\n",
      "0.7367764957271885\n",
      "0.7217598049308779\n",
      "0.7444892000252915\n",
      "0.7400446168736736\n",
      "0.743782252795445\n",
      "0.7326401127094861\n",
      "0.7449712795088252\n",
      "0.7422341996767222\n",
      "0.7480991622987417\n",
      "0.7459995590192718\n",
      "0.7402674580541402\n",
      "0.7487558506955985\n",
      "0.7492861624462758\n",
      "0.7488245918091072\n",
      "0.7485448446596747\n",
      "0.7409200933193041\n",
      "0.7519075658998682\n",
      "0.7493179389987468\n",
      "0.7555300928815658\n",
      "0.7524696542030165\n",
      "0.7525747924391261\n",
      "0.7539238367917356\n",
      "0.7524901630493818\n",
      "0.7529471131164205\n",
      "0.7562747342361549\n",
      "0.7491717830699067\n",
      "0.751623198180306\n",
      "0.749188968348284\n",
      "0.7488199712389775\n",
      "0.7522891277174222\n",
      "0.7534794514329441\n",
      "0.7535455985421696\n",
      "0.7513719040153566\n",
      "0.7540792338608349\n",
      "0.7517614099710282\n",
      "0.7573586227134257\n",
      "0.7500731184957369\n",
      "0.7610711292187021\n",
      "0.7513672834452267\n",
      "0.7542253897896749\n",
      "0.7544046192731275\n",
      "0.6221312339192\n",
      "0.6771646560431384\n",
      "0.6418373170213697\n",
      "0.6971410830292134\n",
      "0.6966946711045756\n",
      "0.7163305539658272\n",
      "0.7221413667484318\n",
      "0.714759965434893\n",
      "0.7407059258406601\n",
      "0.7267228646886303\n",
      "0.7363103045198904\n",
      "0.7321924037827068\n",
      "0.7421898584160037\n",
      "0.7444289704883376\n",
      "0.7451325941501961\n",
      "0.7380111607034291\n",
      "0.7448151528760212\n",
      "0.7455697649345743\n",
      "0.7416985377922105\n",
      "0.7495494538810357\n",
      "0.7405200492212313\n",
      "0.748714833002868\n",
      "0.7493536065576428\n",
      "0.7455518500924925\n",
      "0.7482109476708274\n",
      "0.745778014840947\n",
      "0.7477327591937186\n",
      "0.7504057184824426\n",
      "0.7525324777442539\n",
      "0.7523016924256696\n",
      "0.7472129855854425\n",
      "0.7530714631967537\n",
      "0.7578407021969595\n",
      "0.7577765816535804\n",
      "0.753736663170165\n",
      "0.7550500399638785\n",
      "0.752364515966907\n",
      "0.7603456997083367\n",
      "0.7579094433104683\n",
      "0.7556703312381345\n",
      "0.7560221430690637\n",
      "0.752339386550412\n",
      "0.7553482693938298\n",
      "0.7570034873145084\n",
      "0.7567337108690402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Fall23 Coursework\\ELEC478\\Competition\\elecfinal\\Didi\\QUICK_SUBMISSIONS.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Fall23%20Coursework/ELEC478/Competition/elecfinal/Didi/QUICK_SUBMISSIONS.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m num_features:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Fall23%20Coursework/ELEC478/Competition/elecfinal/Didi/QUICK_SUBMISSIONS.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     RF \u001b[39m=\u001b[39m RandomForestClassifier(max_depth\u001b[39m=\u001b[39mx, max_features \u001b[39m=\u001b[39m n) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Fall23%20Coursework/ELEC478/Competition/elecfinal/Didi/QUICK_SUBMISSIONS.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     RF\u001b[39m.\u001b[39;49mfit(X_train_feat\u001b[39m.\u001b[39;49mdrop(\u001b[39m\"\u001b[39;49m\u001b[39mID\u001b[39;49m\u001b[39m\"\u001b[39;49m, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m),y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Fall23%20Coursework/ELEC478/Competition/elecfinal/Didi/QUICK_SUBMISSIONS.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     y_hat_valid \u001b[39m=\u001b[39m RF\u001b[39m.\u001b[39mpredict(X_val_feat\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mID\u001b[39m\u001b[39m\"\u001b[39m, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Fall23%20Coursework/ELEC478/Competition/elecfinal/Didi/QUICK_SUBMISSIONS.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     valid_errors[(x,n)] \u001b[39m=\u001b[39m balanced_accuracy_score(y_val, y_hat_valid)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_depth_list = np.linspace(1,10,dtype=int)\n",
    "num_features = np.linspace(1,20,dtype = int)\n",
    "\n",
    "valid_errors = {}\n",
    "\n",
    "for x in max_depth_list:\n",
    "    for n in num_features:\n",
    "        RF = RandomForestClassifier(max_depth=x, max_features = n) \n",
    "        RF.fit(X_train_feat.drop(\"ID\", axis = 1),y_train)\n",
    "        y_hat_valid = RF.predict(X_val_feat.drop(\"ID\", axis = 1))\n",
    "        valid_errors[(x,n)] = balanced_accuracy_score(y_val, y_hat_valid)\n",
    "        print(valid_errors[(x,n)])\n",
    "best_params= max(valid_errors, key=valid_errors.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(max_depth=best_params[0], max_features = best_params[1]) \n",
    "RF.fit(X_train_feat.drop(\"ID\", axis = 1),y_train)\n",
    "y_hat_test= RF.predict(X_query_feat.drop(\"ID\", axis = 1))\n",
    "test_acc = balanced_accuracy_score(y_query, y_hat_test)\n",
    "print(f\"The test accuracy (using query set) for Random Forest was {test_acc} with a max depth of {best_params[0]} and a number of features at each split of {best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_predictions = RF.predict(sub_data.drop(\"ID\", axis = 1))\n",
    "sub_data[\"connected\"] = leaderboard_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = sub_data.filter(['ID','connected'])\n",
    "submission_data.to_csv('submission_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
