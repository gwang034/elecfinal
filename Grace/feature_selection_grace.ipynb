{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "sys.path.insert(0, '/Users/gracewang/Documents/GitHub/elecfinal')\n",
    "sys.path.insert(0, 'D:\\Fall23 Coursework\\ELEC478\\Competition\\elecfinal')\n",
    "from ml_pipeline import train_n_predict, validation, clean_split\n",
    "from Data.data_cleaner import cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "feature_path = \"../Data/feature_weights.csv\"\n",
    "morph_path = \"../Data/imputed_morph_embed.csv\"\n",
    "X_train, X_val, X_query, y_train, y_val, y_query = clean_split(train_path, feature_path, morph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample X_train_feat\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "X_train, y_train = ros.fit_resample(\n",
    "        X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = X_train.copy()\n",
    "X_val_feat = X_val.copy()\n",
    "X_query_feat = X_query.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Most Important FW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>feature_weight_0</th>\n",
       "      <th>feature_weight_1</th>\n",
       "      <th>feature_weight_2</th>\n",
       "      <th>feature_weight_3</th>\n",
       "      <th>feature_weight_4</th>\n",
       "      <th>feature_weight_5</th>\n",
       "      <th>feature_weight_6</th>\n",
       "      <th>feature_weight_7</th>\n",
       "      <th>feature_weight_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_weight_502</th>\n",
       "      <th>feature_weight_503</th>\n",
       "      <th>feature_weight_504</th>\n",
       "      <th>feature_weight_505</th>\n",
       "      <th>feature_weight_506</th>\n",
       "      <th>feature_weight_507</th>\n",
       "      <th>feature_weight_508</th>\n",
       "      <th>feature_weight_509</th>\n",
       "      <th>feature_weight_510</th>\n",
       "      <th>feature_weight_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557058</td>\n",
       "      <td>0.205733</td>\n",
       "      <td>0.317099</td>\n",
       "      <td>-0.241713</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>-0.314002</td>\n",
       "      <td>-0.432338</td>\n",
       "      <td>0.152562</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.140944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034500</td>\n",
       "      <td>0.363631</td>\n",
       "      <td>0.183755</td>\n",
       "      <td>0.231815</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>0.402666</td>\n",
       "      <td>0.016072</td>\n",
       "      <td>0.072968</td>\n",
       "      <td>-0.452475</td>\n",
       "      <td>-0.175632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155650</td>\n",
       "      <td>0.166665</td>\n",
       "      <td>-0.293123</td>\n",
       "      <td>-0.017896</td>\n",
       "      <td>-0.159223</td>\n",
       "      <td>0.246045</td>\n",
       "      <td>-0.138767</td>\n",
       "      <td>0.217365</td>\n",
       "      <td>-0.174787</td>\n",
       "      <td>-0.050760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276523</td>\n",
       "      <td>0.333738</td>\n",
       "      <td>0.277954</td>\n",
       "      <td>-0.124248</td>\n",
       "      <td>0.228173</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.198626</td>\n",
       "      <td>-0.044919</td>\n",
       "      <td>0.661572</td>\n",
       "      <td>0.110016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425987</td>\n",
       "      <td>0.453440</td>\n",
       "      <td>0.174334</td>\n",
       "      <td>0.279131</td>\n",
       "      <td>0.144052</td>\n",
       "      <td>0.218536</td>\n",
       "      <td>0.110859</td>\n",
       "      <td>-0.090576</td>\n",
       "      <td>0.125406</td>\n",
       "      <td>0.110690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300525</td>\n",
       "      <td>-0.160008</td>\n",
       "      <td>0.052873</td>\n",
       "      <td>0.112233</td>\n",
       "      <td>-0.091779</td>\n",
       "      <td>-0.046790</td>\n",
       "      <td>0.280856</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.305763</td>\n",
       "      <td>0.218547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262149</td>\n",
       "      <td>0.537410</td>\n",
       "      <td>0.165957</td>\n",
       "      <td>0.543808</td>\n",
       "      <td>-0.386137</td>\n",
       "      <td>-0.113595</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>0.054061</td>\n",
       "      <td>0.247789</td>\n",
       "      <td>-0.502975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113184</td>\n",
       "      <td>0.117278</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.491052</td>\n",
       "      <td>-0.161192</td>\n",
       "      <td>0.868506</td>\n",
       "      <td>0.215711</td>\n",
       "      <td>0.108492</td>\n",
       "      <td>-0.607824</td>\n",
       "      <td>0.296478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>557067</td>\n",
       "      <td>-0.193376</td>\n",
       "      <td>-0.058421</td>\n",
       "      <td>-0.074607</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>-0.038007</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>-0.141879</td>\n",
       "      <td>0.308456</td>\n",
       "      <td>0.392383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>0.413605</td>\n",
       "      <td>-0.092548</td>\n",
       "      <td>-0.091461</td>\n",
       "      <td>-0.025144</td>\n",
       "      <td>-0.202813</td>\n",
       "      <td>0.241059</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>-0.073372</td>\n",
       "      <td>-0.242132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>368622</td>\n",
       "      <td>-0.330541</td>\n",
       "      <td>0.475290</td>\n",
       "      <td>0.010314</td>\n",
       "      <td>0.532330</td>\n",
       "      <td>-0.475708</td>\n",
       "      <td>0.443309</td>\n",
       "      <td>0.129722</td>\n",
       "      <td>-0.246515</td>\n",
       "      <td>0.317328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.001144</td>\n",
       "      <td>0.316298</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.049807</td>\n",
       "      <td>-0.024745</td>\n",
       "      <td>0.322744</td>\n",
       "      <td>-0.502619</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>-0.048334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>614382</td>\n",
       "      <td>0.275322</td>\n",
       "      <td>0.435353</td>\n",
       "      <td>0.091335</td>\n",
       "      <td>-0.033993</td>\n",
       "      <td>-0.504363</td>\n",
       "      <td>-0.133884</td>\n",
       "      <td>-0.367399</td>\n",
       "      <td>0.478917</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121871</td>\n",
       "      <td>-0.017587</td>\n",
       "      <td>0.141196</td>\n",
       "      <td>0.140291</td>\n",
       "      <td>0.354196</td>\n",
       "      <td>-0.044375</td>\n",
       "      <td>0.495297</td>\n",
       "      <td>0.369650</td>\n",
       "      <td>0.114305</td>\n",
       "      <td>0.281917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>557041</td>\n",
       "      <td>-0.598981</td>\n",
       "      <td>0.061932</td>\n",
       "      <td>-0.074026</td>\n",
       "      <td>0.171314</td>\n",
       "      <td>-0.250348</td>\n",
       "      <td>0.438974</td>\n",
       "      <td>-0.235911</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>-0.141897</td>\n",
       "      <td>-0.232804</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.542075</td>\n",
       "      <td>0.380731</td>\n",
       "      <td>0.189838</td>\n",
       "      <td>0.150242</td>\n",
       "      <td>-0.222899</td>\n",
       "      <td>-0.032958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>196596</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.314906</td>\n",
       "      <td>-0.302266</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.024825</td>\n",
       "      <td>0.432788</td>\n",
       "      <td>-0.238054</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218091</td>\n",
       "      <td>0.377625</td>\n",
       "      <td>0.157004</td>\n",
       "      <td>0.265006</td>\n",
       "      <td>0.412147</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.780171</td>\n",
       "      <td>0.079002</td>\n",
       "      <td>-0.009118</td>\n",
       "      <td>0.244956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>188408</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>-0.089854</td>\n",
       "      <td>0.038044</td>\n",
       "      <td>0.238926</td>\n",
       "      <td>0.198330</td>\n",
       "      <td>0.317643</td>\n",
       "      <td>-0.111727</td>\n",
       "      <td>-0.440526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135782</td>\n",
       "      <td>0.255072</td>\n",
       "      <td>-0.291722</td>\n",
       "      <td>0.568217</td>\n",
       "      <td>0.409256</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.364912</td>\n",
       "      <td>0.098059</td>\n",
       "      <td>0.057065</td>\n",
       "      <td>-0.119618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2692 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nucleus_id  feature_weight_0  feature_weight_1  feature_weight_2  \\\n",
       "0         557058          0.205733          0.317099         -0.241713   \n",
       "1         155650          0.166665         -0.293123         -0.017896   \n",
       "2         425987          0.453440          0.174334          0.279131   \n",
       "3         262149          0.537410          0.165957          0.543808   \n",
       "4         557067         -0.193376         -0.058421         -0.074607   \n",
       "...          ...               ...               ...               ...   \n",
       "2687      368622         -0.330541          0.475290          0.010314   \n",
       "2688      614382          0.275322          0.435353          0.091335   \n",
       "2689      557041         -0.598981          0.061932         -0.074026   \n",
       "2690      196596          0.024009          0.075810          0.314906   \n",
       "2691      188408         -0.421826         -0.085335         -0.089854   \n",
       "\n",
       "      feature_weight_3  feature_weight_4  feature_weight_5  feature_weight_6  \\\n",
       "0             0.014264         -0.314002         -0.432338          0.152562   \n",
       "1            -0.159223          0.246045         -0.138767          0.217365   \n",
       "2             0.144052          0.218536          0.110859         -0.090576   \n",
       "3            -0.386137         -0.113595          0.046351          0.054061   \n",
       "4             0.009510         -0.038007          0.011739         -0.141879   \n",
       "...                ...               ...               ...               ...   \n",
       "2687          0.532330         -0.475708          0.443309          0.129722   \n",
       "2688         -0.033993         -0.504363         -0.133884         -0.367399   \n",
       "2689          0.171314         -0.250348          0.438974         -0.235911   \n",
       "2690         -0.302266          0.007095          0.024825          0.432788   \n",
       "2691          0.038044          0.238926          0.198330          0.317643   \n",
       "\n",
       "      feature_weight_7  feature_weight_8  ...  feature_weight_502  \\\n",
       "0             0.186511          0.140944  ...           -0.034500   \n",
       "1            -0.174787         -0.050760  ...            0.276523   \n",
       "2             0.125406          0.110690  ...            0.300525   \n",
       "3             0.247789         -0.502975  ...           -0.113184   \n",
       "4             0.308456          0.392383  ...            0.327086   \n",
       "...                ...               ...  ...                 ...   \n",
       "2687         -0.246515          0.317328  ...           -0.000365   \n",
       "2688          0.478917          0.033984  ...            0.121871   \n",
       "2689          0.036702          0.365656  ...            0.039513   \n",
       "2690         -0.238054          0.210615  ...            0.218091   \n",
       "2691         -0.111727         -0.440526  ...           -0.135782   \n",
       "\n",
       "      feature_weight_503  feature_weight_504  feature_weight_505  \\\n",
       "0               0.363631            0.183755            0.231815   \n",
       "1               0.333738            0.277954           -0.124248   \n",
       "2              -0.160008            0.052873            0.112233   \n",
       "3               0.117278            0.011819            0.491052   \n",
       "4               0.413605           -0.092548           -0.091461   \n",
       "...                  ...                 ...                 ...   \n",
       "2687           -0.001144            0.316298            0.075985   \n",
       "2688           -0.017587            0.141196            0.140291   \n",
       "2689           -0.141897           -0.232804            0.020774   \n",
       "2690            0.377625            0.157004            0.265006   \n",
       "2691            0.255072           -0.291722            0.568217   \n",
       "\n",
       "      feature_weight_506  feature_weight_507  feature_weight_508  \\\n",
       "0               0.042877            0.402666            0.016072   \n",
       "1               0.228173            0.029465            0.198626   \n",
       "2              -0.091779           -0.046790            0.280856   \n",
       "3              -0.161192            0.868506            0.215711   \n",
       "4              -0.025144           -0.202813            0.241059   \n",
       "...                  ...                 ...                 ...   \n",
       "2687            0.049807           -0.024745            0.322744   \n",
       "2688            0.354196           -0.044375            0.495297   \n",
       "2689            0.542075            0.380731            0.189838   \n",
       "2690            0.412147            0.088211            0.780171   \n",
       "2691            0.409256           -0.102047            0.364912   \n",
       "\n",
       "      feature_weight_509  feature_weight_510  feature_weight_511  \n",
       "0               0.072968           -0.452475           -0.175632  \n",
       "1              -0.044919            0.661572            0.110016  \n",
       "2               0.399340            0.305763            0.218547  \n",
       "3               0.108492           -0.607824            0.296478  \n",
       "4               0.014646           -0.073372           -0.242132  \n",
       "...                  ...                 ...                 ...  \n",
       "2687           -0.502619            0.218220           -0.048334  \n",
       "2688            0.369650            0.114305            0.281917  \n",
       "2689            0.150242           -0.222899           -0.032958  \n",
       "2690            0.079002           -0.009118            0.244956  \n",
       "2691            0.098059            0.057065           -0.119618  \n",
       "\n",
       "[2692 rows x 513 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fw = X_train.copy() # make a copy of the original X_train\n",
    "fw = pd.read_csv(feature_path)\n",
    "X_train_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>feature_weight_0</th>\n",
       "      <th>feature_weight_1</th>\n",
       "      <th>feature_weight_2</th>\n",
       "      <th>feature_weight_3</th>\n",
       "      <th>feature_weight_4</th>\n",
       "      <th>feature_weight_5</th>\n",
       "      <th>feature_weight_6</th>\n",
       "      <th>feature_weight_7</th>\n",
       "      <th>feature_weight_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_weight_503</th>\n",
       "      <th>feature_weight_504</th>\n",
       "      <th>feature_weight_505</th>\n",
       "      <th>feature_weight_506</th>\n",
       "      <th>feature_weight_507</th>\n",
       "      <th>feature_weight_508</th>\n",
       "      <th>feature_weight_509</th>\n",
       "      <th>feature_weight_510</th>\n",
       "      <th>feature_weight_511</th>\n",
       "      <th>pre_nucleus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557121</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>-0.510590</td>\n",
       "      <td>0.714646</td>\n",
       "      <td>-0.232921</td>\n",
       "      <td>-0.207754</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.369591</td>\n",
       "      <td>-0.367968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123481</td>\n",
       "      <td>-0.214475</td>\n",
       "      <td>-0.599043</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.522783</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.146128</td>\n",
       "      <td>557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>557121</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>-0.510590</td>\n",
       "      <td>0.714646</td>\n",
       "      <td>-0.232921</td>\n",
       "      <td>-0.207754</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.369591</td>\n",
       "      <td>-0.367968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123481</td>\n",
       "      <td>-0.214475</td>\n",
       "      <td>-0.599043</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.522783</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.146128</td>\n",
       "      <td>557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>557121</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>-0.510590</td>\n",
       "      <td>0.714646</td>\n",
       "      <td>-0.232921</td>\n",
       "      <td>-0.207754</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.369591</td>\n",
       "      <td>-0.367968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123481</td>\n",
       "      <td>-0.214475</td>\n",
       "      <td>-0.599043</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.522783</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.146128</td>\n",
       "      <td>557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557121</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>-0.510590</td>\n",
       "      <td>0.714646</td>\n",
       "      <td>-0.232921</td>\n",
       "      <td>-0.207754</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.369591</td>\n",
       "      <td>-0.367968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123481</td>\n",
       "      <td>-0.214475</td>\n",
       "      <td>-0.599043</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.522783</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.146128</td>\n",
       "      <td>557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>557121</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>-0.510590</td>\n",
       "      <td>0.714646</td>\n",
       "      <td>-0.232921</td>\n",
       "      <td>-0.207754</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.369591</td>\n",
       "      <td>-0.367968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123481</td>\n",
       "      <td>-0.214475</td>\n",
       "      <td>-0.599043</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.522783</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.146128</td>\n",
       "      <td>557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239065</th>\n",
       "      <td>557030</td>\n",
       "      <td>-0.218660</td>\n",
       "      <td>-0.213865</td>\n",
       "      <td>-0.242872</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>0.496228</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>-0.381078</td>\n",
       "      <td>-0.145225</td>\n",
       "      <td>0.364091</td>\n",
       "      <td>557030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239066</th>\n",
       "      <td>557030</td>\n",
       "      <td>-0.218660</td>\n",
       "      <td>-0.213865</td>\n",
       "      <td>-0.242872</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>0.496228</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>-0.381078</td>\n",
       "      <td>-0.145225</td>\n",
       "      <td>0.364091</td>\n",
       "      <td>557030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239067</th>\n",
       "      <td>557030</td>\n",
       "      <td>-0.218660</td>\n",
       "      <td>-0.213865</td>\n",
       "      <td>-0.242872</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>0.496228</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>-0.381078</td>\n",
       "      <td>-0.145225</td>\n",
       "      <td>0.364091</td>\n",
       "      <td>557030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239068</th>\n",
       "      <td>557030</td>\n",
       "      <td>-0.218660</td>\n",
       "      <td>-0.213865</td>\n",
       "      <td>-0.242872</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>0.496228</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>-0.381078</td>\n",
       "      <td>-0.145225</td>\n",
       "      <td>0.364091</td>\n",
       "      <td>557030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239069</th>\n",
       "      <td>557030</td>\n",
       "      <td>-0.218660</td>\n",
       "      <td>-0.213865</td>\n",
       "      <td>-0.242872</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164836</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>0.496228</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>-0.381078</td>\n",
       "      <td>-0.145225</td>\n",
       "      <td>0.364091</td>\n",
       "      <td>557030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239070 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        nucleus_id  feature_weight_0  feature_weight_1  feature_weight_2  \\\n",
       "0           557121          0.408289          0.051098         -0.510590   \n",
       "1           557121          0.408289          0.051098         -0.510590   \n",
       "2           557121          0.408289          0.051098         -0.510590   \n",
       "3           557121          0.408289          0.051098         -0.510590   \n",
       "4           557121          0.408289          0.051098         -0.510590   \n",
       "...            ...               ...               ...               ...   \n",
       "239065      557030         -0.218660         -0.213865         -0.242872   \n",
       "239066      557030         -0.218660         -0.213865         -0.242872   \n",
       "239067      557030         -0.218660         -0.213865         -0.242872   \n",
       "239068      557030         -0.218660         -0.213865         -0.242872   \n",
       "239069      557030         -0.218660         -0.213865         -0.242872   \n",
       "\n",
       "        feature_weight_3  feature_weight_4  feature_weight_5  \\\n",
       "0               0.714646         -0.232921         -0.207754   \n",
       "1               0.714646         -0.232921         -0.207754   \n",
       "2               0.714646         -0.232921         -0.207754   \n",
       "3               0.714646         -0.232921         -0.207754   \n",
       "4               0.714646         -0.232921         -0.207754   \n",
       "...                  ...               ...               ...   \n",
       "239065          0.169476         -0.038819          0.442058   \n",
       "239066          0.169476         -0.038819          0.442058   \n",
       "239067          0.169476         -0.038819          0.442058   \n",
       "239068          0.169476         -0.038819          0.442058   \n",
       "239069          0.169476         -0.038819          0.442058   \n",
       "\n",
       "        feature_weight_6  feature_weight_7  feature_weight_8  ...  \\\n",
       "0              -0.030060          0.369591         -0.367968  ...   \n",
       "1              -0.030060          0.369591         -0.367968  ...   \n",
       "2              -0.030060          0.369591         -0.367968  ...   \n",
       "3              -0.030060          0.369591         -0.367968  ...   \n",
       "4              -0.030060          0.369591         -0.367968  ...   \n",
       "...                  ...               ...               ...  ...   \n",
       "239065          0.012513          0.466563          0.399331  ...   \n",
       "239066          0.012513          0.466563          0.399331  ...   \n",
       "239067          0.012513          0.466563          0.399331  ...   \n",
       "239068          0.012513          0.466563          0.399331  ...   \n",
       "239069          0.012513          0.466563          0.399331  ...   \n",
       "\n",
       "        feature_weight_503  feature_weight_504  feature_weight_505  \\\n",
       "0                -0.123481           -0.214475           -0.599043   \n",
       "1                -0.123481           -0.214475           -0.599043   \n",
       "2                -0.123481           -0.214475           -0.599043   \n",
       "3                -0.123481           -0.214475           -0.599043   \n",
       "4                -0.123481           -0.214475           -0.599043   \n",
       "...                    ...                 ...                 ...   \n",
       "239065            0.164836           -0.155090            0.051676   \n",
       "239066            0.164836           -0.155090            0.051676   \n",
       "239067            0.164836           -0.155090            0.051676   \n",
       "239068            0.164836           -0.155090            0.051676   \n",
       "239069            0.164836           -0.155090            0.051676   \n",
       "\n",
       "        feature_weight_506  feature_weight_507  feature_weight_508  \\\n",
       "0                 0.072844            0.522783            0.015744   \n",
       "1                 0.072844            0.522783            0.015744   \n",
       "2                 0.072844            0.522783            0.015744   \n",
       "3                 0.072844            0.522783            0.015744   \n",
       "4                 0.072844            0.522783            0.015744   \n",
       "...                    ...                 ...                 ...   \n",
       "239065            0.496228            0.104460            0.101426   \n",
       "239066            0.496228            0.104460            0.101426   \n",
       "239067            0.496228            0.104460            0.101426   \n",
       "239068            0.496228            0.104460            0.101426   \n",
       "239069            0.496228            0.104460            0.101426   \n",
       "\n",
       "        feature_weight_509  feature_weight_510  feature_weight_511  \\\n",
       "0                 0.359440           -0.021606           -0.146128   \n",
       "1                 0.359440           -0.021606           -0.146128   \n",
       "2                 0.359440           -0.021606           -0.146128   \n",
       "3                 0.359440           -0.021606           -0.146128   \n",
       "4                 0.359440           -0.021606           -0.146128   \n",
       "...                    ...                 ...                 ...   \n",
       "239065           -0.381078           -0.145225            0.364091   \n",
       "239066           -0.381078           -0.145225            0.364091   \n",
       "239067           -0.381078           -0.145225            0.364091   \n",
       "239068           -0.381078           -0.145225            0.364091   \n",
       "239069           -0.381078           -0.145225            0.364091   \n",
       "\n",
       "        pre_nucleus_id  \n",
       "0               557121  \n",
       "1               557121  \n",
       "2               557121  \n",
       "3               557121  \n",
       "4               557121  \n",
       "...                ...  \n",
       "239065          557030  \n",
       "239066          557030  \n",
       "239067          557030  \n",
       "239068          557030  \n",
       "239069          557030  \n",
       "\n",
       "[239070 rows x 514 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a dataframe of feature weights for each obs. in X\n",
    "X_train_fw = fw.merge(X_train_fw['pre_nucleus_id'], right_on ='pre_nucleus_id', left_on='nucleus_id')\n",
    "X_val_fw = fw.merge(X_val['pre_nucleus_id'], right_on ='pre_nucleus_id', left_on='nucleus_id')\n",
    "X_train_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5039385696827658"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_model = LogisticRegression(max_iter=500, solver='saga').fit(X_train_fw.drop(columns=['pre_nucleus_id', 'nucleus_id']).sort_index(axis=1), y_train)\n",
    "preds = fw_model.predict(X_val_fw.drop(columns=['nucleus_id', 'pre_nucleus_id']).sort_index(axis=1))\n",
    "balanced_accuracy_score(y_val,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87905109e-01, -1.59261052e-04, -4.11382870e-01,\n",
       "        -9.04597872e-02,  7.74336663e-02, -1.84727554e-01,\n",
       "        -6.31668802e-01, -8.41690600e-02, -7.06762806e-01,\n",
       "         1.15974377e-01,  5.02110274e-01,  4.46436911e-01,\n",
       "         1.09576016e-01, -5.96001979e-01, -1.19420203e-01,\n",
       "         1.04446698e-01, -4.35287300e-01,  2.81188618e-01,\n",
       "         1.23099389e-01,  3.02337789e-01,  4.94687974e-01,\n",
       "         1.07056915e-01,  1.14215817e-01,  1.86713241e-01,\n",
       "         2.54069941e-01, -4.38175894e-01, -3.00779466e-01,\n",
       "         3.53740155e-01,  3.53836334e-02, -5.30516185e-01,\n",
       "        -2.08428794e-01, -4.17765191e-01,  3.10133583e-01,\n",
       "        -2.89535942e-01, -3.31267971e-01,  2.80790467e-01,\n",
       "         1.89520012e-01, -4.58491034e-02,  5.98317267e-02,\n",
       "         4.89907858e-01, -2.08101837e-01, -1.45131123e-01,\n",
       "         7.31704104e-01, -4.66197202e-01,  2.66403867e-01,\n",
       "         2.88544324e-01,  1.64289603e-01, -2.38119692e-01,\n",
       "        -2.75648512e-01,  6.00507555e-01, -2.84215078e-01,\n",
       "        -2.12918377e-03,  1.84414146e-01, -8.56413959e-02,\n",
       "         2.48091328e-01,  4.95083063e-02, -1.26816701e-01,\n",
       "         3.40193593e-01,  6.27757758e-01, -1.02430694e-01,\n",
       "         1.04665530e-01, -7.82483998e-01,  1.96270442e-01,\n",
       "         1.55169439e-01, -2.49564448e-01,  5.09055595e-01,\n",
       "        -3.75140903e-01, -4.97660656e-01, -2.77203895e-01,\n",
       "         1.20828390e-01,  6.46489404e-01,  4.27395233e-01,\n",
       "        -6.16118141e-01,  7.21242017e-01,  5.69497263e-01,\n",
       "        -3.85047272e-01,  4.14880508e-01, -3.23878781e-01,\n",
       "        -3.54319118e-01,  2.24807559e-01, -1.38729061e-01,\n",
       "        -6.45468170e-01, -5.15111187e-01,  3.69024236e-01,\n",
       "        -4.33559298e-02,  2.73670343e-01,  1.88534043e-01,\n",
       "        -1.94791581e-01,  6.74854787e-02,  2.05449358e-01,\n",
       "        -3.20154760e-01, -7.44251581e-01,  1.91647419e-01,\n",
       "        -8.47666488e-01, -3.80054935e-02, -2.97055584e-01,\n",
       "        -3.03215090e-01,  2.56112750e-01, -7.15632235e-02,\n",
       "         4.38029354e-01,  1.76546077e-02,  7.16044211e-01,\n",
       "         9.65333387e-01, -1.71637686e-01,  5.52315520e-01,\n",
       "        -6.16519981e-01,  3.30784171e-01,  9.36775929e-02,\n",
       "         3.70711308e-01, -2.90074277e-01,  7.77361304e-01,\n",
       "         5.17363838e-01, -2.53295473e-01,  5.09308063e-02,\n",
       "        -3.32165926e-01,  1.82935690e-01, -1.50891058e-01,\n",
       "         2.38434868e-01, -5.88066801e-02, -6.33836501e-01,\n",
       "        -4.85314826e-01, -1.94968672e-01,  4.54091061e-01,\n",
       "         3.88888578e-01, -1.38918735e-01,  5.60787676e-01,\n",
       "         2.30072000e-01,  2.46773549e-02,  2.04203001e-01,\n",
       "         6.18900818e-01, -1.21224393e-01, -8.01455887e-02,\n",
       "        -5.93535486e-03, -2.25756856e-01, -2.89422021e-01,\n",
       "        -4.35682931e-01,  1.03258740e-01,  2.76306767e-01,\n",
       "        -2.24487373e-01,  2.49068831e-01,  2.45237788e-01,\n",
       "         2.32587071e-01, -4.77683848e-01, -6.93490064e-01,\n",
       "        -2.45997359e-01,  4.81730831e-01, -2.28104500e-01,\n",
       "        -3.12153224e-01, -3.85746699e-01, -1.02925468e-01,\n",
       "         3.59292342e-01,  4.53180495e-01, -1.79582924e-01,\n",
       "         6.02747968e-01, -3.06708265e-01, -3.22039685e-01,\n",
       "         5.89183988e-02, -4.78080068e-01,  2.60654931e-01,\n",
       "         1.68228822e-01,  2.27758806e-01, -2.30813082e-01,\n",
       "         5.24240816e-01,  1.53335534e-01, -5.94626028e-01,\n",
       "        -9.73182541e-02, -7.99545749e-01, -2.03084784e-02,\n",
       "        -3.66084134e-01, -3.29030568e-01,  2.41119968e-01,\n",
       "        -1.52713474e-01,  1.78955041e-01, -1.51511949e-01,\n",
       "        -5.49226462e-01,  2.44221817e-01,  8.85329801e-01,\n",
       "        -4.47239870e-01,  7.38313308e-01, -1.90777389e-01,\n",
       "         6.14507350e-01, -3.06295894e-01,  2.27970342e-01,\n",
       "         1.20158703e-01, -2.55927875e-01,  6.11981421e-01,\n",
       "         4.69064294e-01, -3.59454983e-01, -4.58737702e-01,\n",
       "        -2.29137442e-01, -2.23720112e-01,  1.68231518e-01,\n",
       "        -3.04130215e-01,  2.62048151e-01, -5.28202860e-02,\n",
       "         2.79186277e-01,  4.68093744e-01,  7.00919190e-01,\n",
       "         3.84858936e-01, -7.54039106e-01, -6.29792671e-01,\n",
       "        -2.49519822e-01, -3.49474721e-01, -4.31815381e-01,\n",
       "         2.09618109e-01,  2.25956516e-01, -2.02812929e-01,\n",
       "         2.52136186e-01, -3.46096876e-02,  9.52305030e-02,\n",
       "         5.41399955e-01, -1.58279181e-01,  1.82074554e-01,\n",
       "        -2.91578031e-01, -1.86732892e-02, -2.85426821e-01,\n",
       "         7.48537695e-02,  4.17769054e-01,  1.68129479e-01,\n",
       "         7.90518651e-01,  4.12332693e-02,  2.43548611e-02,\n",
       "        -3.12163709e-01, -2.51107478e-01, -7.22694883e-01,\n",
       "         3.83615357e-01,  1.20916542e-02, -5.23288774e-01,\n",
       "         1.45495504e-02, -3.70063863e-01, -4.63399229e-01,\n",
       "        -4.48823063e-01, -1.91884120e-01, -1.06766730e+00,\n",
       "        -1.88440572e-01,  7.51715653e-02, -6.89318544e-01,\n",
       "        -3.64957722e-01, -1.26791590e-02, -6.23053852e-01,\n",
       "        -9.28182316e-02, -1.54446191e-01,  5.48652541e-02,\n",
       "        -3.05610317e-01,  1.77095723e-01,  1.25870388e-01,\n",
       "         6.99560497e-01, -2.42155834e-04,  2.11009909e-01,\n",
       "         1.96493271e-01,  1.42192792e-01,  7.64895244e-02,\n",
       "        -6.55999803e-02, -1.57164497e-01,  8.30671091e-03,\n",
       "        -6.92754480e-01, -3.44497553e-01,  7.89575183e-01,\n",
       "        -3.67992475e-01,  9.84743602e-03,  3.56516784e-01,\n",
       "         5.53289278e-01, -2.81064836e-01, -2.69477424e-01,\n",
       "        -3.23892009e-03,  3.28419434e-01, -1.05160092e-01,\n",
       "         3.10094115e-02,  5.54247735e-01, -2.00995880e-01,\n",
       "         3.83205406e-01,  2.64957932e-01, -2.26425935e-01,\n",
       "        -4.18025457e-02,  5.77887582e-01, -2.45117310e-01,\n",
       "         1.78444451e-01, -3.29308799e-01, -2.16661887e-01,\n",
       "        -5.12042163e-02,  6.73587793e-01, -1.84850899e-01,\n",
       "         2.33213056e-01,  4.41294198e-01,  4.22982550e-02,\n",
       "         1.06318538e-01, -9.81434141e-02, -1.17016325e-01,\n",
       "         2.26502574e-02,  4.84922859e-02,  1.44080697e-01,\n",
       "        -9.28734535e-02,  2.62337000e-01, -2.33570892e-03,\n",
       "        -4.86746004e-01,  6.30676590e-01,  7.06050678e-01,\n",
       "         1.78857321e-01,  3.90751060e-01,  5.10137347e-01,\n",
       "         1.34975787e-02, -6.40617993e-01, -4.54916714e-02,\n",
       "        -2.10881662e-01,  5.09375409e-01,  1.97468852e-02,\n",
       "         3.49076378e-02,  2.94729252e-01, -2.28339632e-01,\n",
       "        -4.83365914e-01, -1.14374015e+00,  8.21893194e-01,\n",
       "         3.30951364e-02, -2.29209388e-01, -3.99399805e-01,\n",
       "         2.21531997e-01,  1.39162074e-01,  8.24102994e-02,\n",
       "         5.20004070e-01, -2.08939917e-02, -5.06068466e-01,\n",
       "         1.91009425e-01, -1.91274658e-01, -5.89761052e-01,\n",
       "        -1.93609324e-01, -1.82061332e-02,  2.03907053e-03,\n",
       "         9.16856100e-01,  1.77904509e-01,  5.26258534e-01,\n",
       "        -3.07817227e-02,  1.82052026e-01,  2.01992328e-01,\n",
       "         7.89710175e-03,  1.76154673e-01, -1.48275214e-01,\n",
       "         3.81697930e-01,  5.09348730e-01, -5.31889211e-01,\n",
       "         3.10323368e-01,  3.16045390e-01,  6.95961485e-01,\n",
       "         2.07735455e-01, -2.17876468e-01,  1.02843430e+00,\n",
       "         3.36346208e-01, -5.90510894e-01, -5.01076507e-01,\n",
       "        -7.55528802e-01,  1.45183046e-01,  4.61288115e-01,\n",
       "        -4.63157782e-01, -2.67442379e-01,  1.01541199e-01,\n",
       "        -1.78940405e-01,  3.09325928e-01, -8.87107559e-02,\n",
       "         2.22939872e-01,  3.88935647e-02, -1.00170972e-01,\n",
       "        -2.72566968e-01, -5.43168278e-01,  6.59148938e-02,\n",
       "        -1.90040584e-01,  3.97092711e-01, -7.70181904e-02,\n",
       "         3.40100218e-01,  6.82666532e-01,  1.55020946e-01,\n",
       "        -7.75097802e-02,  1.15039584e-01,  7.58391305e-02,\n",
       "        -2.60052613e-01,  1.10175931e-01,  2.05199799e-01,\n",
       "         6.26125368e-01, -2.29971813e-01,  1.29649473e-01,\n",
       "         3.87951964e-01,  2.31758271e-02,  3.57992178e-01,\n",
       "         5.42383336e-02,  1.96359662e-01, -2.51205445e-01,\n",
       "        -3.57765069e-01, -8.99749448e-02, -1.28116214e-01,\n",
       "        -5.20243712e-01, -2.62458576e-01,  7.55631592e-01,\n",
       "        -4.76017403e-02, -5.12736439e-01, -3.12423271e-02,\n",
       "         3.42009309e-01,  5.88775151e-02, -1.34183196e-01,\n",
       "         4.68657033e-01,  4.77268420e-01,  9.91035581e-02,\n",
       "        -4.68441086e-03,  4.27673439e-01, -1.35893666e-01,\n",
       "         1.85076398e-01,  7.05754528e-02, -3.20378034e-01,\n",
       "         1.87369608e-01, -7.50606020e-01, -1.44900079e-01,\n",
       "         1.99690295e-01, -3.82767421e-01, -5.54418375e-01,\n",
       "        -4.66788663e-01, -5.58662817e-02,  1.13328388e-01,\n",
       "        -5.72023102e-01,  3.04671291e-01, -8.66192294e-02,\n",
       "        -1.48556174e-01,  4.69114158e-01, -7.73037963e-02,\n",
       "         1.86379860e-01,  4.55749961e-02, -2.94330175e-01,\n",
       "        -3.77206874e-02, -3.61515817e-01,  4.52147340e-02,\n",
       "        -5.06610501e-01, -2.34646181e-01, -4.91052601e-01,\n",
       "        -3.86707737e-01, -3.55082198e-01, -3.33185722e-01,\n",
       "         7.42908562e-02,  2.11351981e-01, -2.83640546e-01,\n",
       "         2.04628261e-01, -1.41718570e-01, -3.80161349e-01,\n",
       "         5.91426223e-01,  2.96454491e-01,  8.26290105e-02,\n",
       "        -1.97593297e-01, -1.53234355e-01,  4.21520189e-01,\n",
       "         2.22697583e-01,  4.69018842e-01, -4.65959466e-01,\n",
       "         5.25971484e-02, -2.94076288e-01, -1.58352190e-04,\n",
       "         7.86491528e-02,  1.00040446e-01,  6.81811280e-02,\n",
       "        -2.72959557e-01,  1.81069583e-01, -5.82958237e-01,\n",
       "        -1.26879827e+00, -7.63442169e-02, -2.49889871e-01,\n",
       "         4.69851027e-02,  1.27417356e-01, -2.23683280e-01,\n",
       "        -2.86067726e-01,  4.38761827e-01,  1.43718094e-01,\n",
       "        -2.42438621e-01, -2.71411850e-01,  2.39178785e-01,\n",
       "         6.94225794e-01,  2.08459847e-01,  2.73832905e-01,\n",
       "         2.14667036e-01,  1.40698233e-01,  1.76026228e-01,\n",
       "        -2.76222103e-01, -2.43319709e-01, -4.04205818e-01,\n",
       "         3.37339222e-01,  1.18191964e-01,  7.54689602e-01,\n",
       "         4.24136970e-01,  2.87630221e-03,  7.50212690e-03,\n",
       "         8.09773664e-01,  6.37776664e-01, -4.21951648e-01,\n",
       "        -3.94702268e-01,  6.98478324e-01, -8.79143443e-02,\n",
       "         5.61862363e-02,  9.87998378e-02, -6.09165590e-01,\n",
       "        -3.37897521e-01, -3.95420806e-01,  4.85443277e-01,\n",
       "         1.55453214e-01, -2.38054965e-01, -4.66575005e-01,\n",
       "         7.75205115e-02,  8.17852495e-01, -4.55634956e-01,\n",
       "        -3.05252802e-02, -7.42678583e-02,  6.87310022e-01,\n",
       "         7.13595231e-01,  1.51227197e-01,  1.04931333e+00,\n",
       "        -5.75874713e-01, -1.38808250e-01,  1.53021212e-01,\n",
       "        -2.98471783e-01,  2.56766065e-01]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>feature_weight_506</td>\n",
       "      <td>1.049313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>feature_weight_344</td>\n",
       "      <td>1.028434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>feature_weight_102</td>\n",
       "      <td>0.965333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>feature_weight_327</td>\n",
       "      <td>0.916856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>feature_weight_176</td>\n",
       "      <td>0.885330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>feature_weight_166</td>\n",
       "      <td>-0.799546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>feature_weight_93</td>\n",
       "      <td>-0.847666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>feature_weight_233</td>\n",
       "      <td>-1.067667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>feature_weight_310</td>\n",
       "      <td>-1.143740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>feature_weight_456</td>\n",
       "      <td>-1.268798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Features  Importance\n",
       "506  feature_weight_506    1.049313\n",
       "344  feature_weight_344    1.028434\n",
       "102  feature_weight_102    0.965333\n",
       "327  feature_weight_327    0.916856\n",
       "176  feature_weight_176    0.885330\n",
       "..                  ...         ...\n",
       "166  feature_weight_166   -0.799546\n",
       "93    feature_weight_93   -0.847666\n",
       "233  feature_weight_233   -1.067667\n",
       "310  feature_weight_310   -1.143740\n",
       "456  feature_weight_456   -1.268798\n",
       "\n",
       "[512 rows x 2 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_df = pd.DataFrame({\"Features\":X_val_fw.drop(columns=['pre_nucleus_id', 'nucleus_id']).columns, \"Importance\":fw_model.coef_[0]})\n",
    "imp_df.sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include FW 101 in the training data\n",
    "def add_feature(df, fw, feature_name):\n",
    "    \"\"\"\"\n",
    "    Adds a feature weight to the dataframe\n",
    "    \"\"\"\n",
    "    df = df.merge(fw[[feature_name, 'nucleus_id']], left_on='pre_nucleus_id', right_on='nucleus_id')\n",
    "    df = df.drop(columns='nucleus_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = add_feature(X_train_feat, fw, 'feature_weight_398')\n",
    "X_val_feat = add_feature(X_val_feat, fw, 'feature_weight_398')\n",
    "X_query_feat = add_feature(X_query_feat, fw, 'feature_weight_398')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>adp_dist</th>\n",
       "      <th>post_skeletal_distance_to_soma</th>\n",
       "      <th>pre_skeletal_distance_to_soma</th>\n",
       "      <th>pre_oracle</th>\n",
       "      <th>pre_test_score</th>\n",
       "      <th>post_oracle</th>\n",
       "      <th>post_test_score</th>\n",
       "      <th>compartment</th>\n",
       "      <th>pre_brain_area</th>\n",
       "      <th>...</th>\n",
       "      <th>post_morph_embeddings</th>\n",
       "      <th>me_similarity</th>\n",
       "      <th>fw_similarity</th>\n",
       "      <th>axonal_coords</th>\n",
       "      <th>dendritic_coords</th>\n",
       "      <th>pre_rf_coords</th>\n",
       "      <th>post_rf_coords</th>\n",
       "      <th>pre_nucleus_coords</th>\n",
       "      <th>nuclei_adp_dist</th>\n",
       "      <th>feature_weight_398</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42593</td>\n",
       "      <td>-1.515697</td>\n",
       "      <td>2.007164</td>\n",
       "      <td>0.931055</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>1.191761</td>\n",
       "      <td>-0.205420</td>\n",
       "      <td>axon</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0723994970321655, -0.7540942430496216, 0.11...</td>\n",
       "      <td>-0.934986</td>\n",
       "      <td>-0.182150</td>\n",
       "      <td>[1187660, 411978, 1089020]</td>\n",
       "      <td>[1187390, 412220, 1089160]</td>\n",
       "      <td>[882.7244675159454, 519.1477453708649]</td>\n",
       "      <td>[858.5207009315491, 608.4342455863953]</td>\n",
       "      <td>[1283392, 621504, 1020280]</td>\n",
       "      <td>0.076496</td>\n",
       "      <td>0.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42594</td>\n",
       "      <td>-1.225578</td>\n",
       "      <td>1.070301</td>\n",
       "      <td>0.268030</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>1.191761</td>\n",
       "      <td>-0.205420</td>\n",
       "      <td>axon</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0723994970321655, -0.7540942430496216, 0.11...</td>\n",
       "      <td>-0.934986</td>\n",
       "      <td>-0.182150</td>\n",
       "      <td>[1204580, 682542, 873138]</td>\n",
       "      <td>[1204640, 682870, 873890]</td>\n",
       "      <td>[882.7244675159454, 519.1477453708649]</td>\n",
       "      <td>[858.5207009315491, 608.4342455863953]</td>\n",
       "      <td>[1283392, 621504, 1020280]</td>\n",
       "      <td>-0.306546</td>\n",
       "      <td>0.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42595</td>\n",
       "      <td>0.632301</td>\n",
       "      <td>2.099967</td>\n",
       "      <td>0.905442</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>1.191761</td>\n",
       "      <td>-0.205420</td>\n",
       "      <td>axon</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0723994970321655, -0.7540942430496216, 0.11...</td>\n",
       "      <td>-0.934986</td>\n",
       "      <td>-0.182150</td>\n",
       "      <td>[1191790, 403683, 1093180]</td>\n",
       "      <td>[1188590, 402414, 1092660]</td>\n",
       "      <td>[882.7244675159454, 519.1477453708649]</td>\n",
       "      <td>[858.5207009315491, 608.4342455863953]</td>\n",
       "      <td>[1283392, 621504, 1020280]</td>\n",
       "      <td>0.118612</td>\n",
       "      <td>0.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42596</td>\n",
       "      <td>0.645635</td>\n",
       "      <td>1.931656</td>\n",
       "      <td>0.956222</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>1.191761</td>\n",
       "      <td>-0.205420</td>\n",
       "      <td>axon</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0723994970321655, -0.7540942430496216, 0.11...</td>\n",
       "      <td>-0.934986</td>\n",
       "      <td>-0.182150</td>\n",
       "      <td>[1184320, 419286, 1082930]</td>\n",
       "      <td>[1186620, 419721, 1085540]</td>\n",
       "      <td>[882.7244675159454, 519.1477453708649]</td>\n",
       "      <td>[858.5207009315491, 608.4342455863953]</td>\n",
       "      <td>[1283392, 621504, 1020280]</td>\n",
       "      <td>0.035778</td>\n",
       "      <td>0.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42597</td>\n",
       "      <td>1.334344</td>\n",
       "      <td>1.668055</td>\n",
       "      <td>-0.042977</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>1.191761</td>\n",
       "      <td>-0.205420</td>\n",
       "      <td>axon</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0723994970321655, -0.7540942430496216, 0.11...</td>\n",
       "      <td>-0.934986</td>\n",
       "      <td>-0.182150</td>\n",
       "      <td>[1189150, 673302, 944202]</td>\n",
       "      <td>[1188790, 677771, 942901]</td>\n",
       "      <td>[882.7244675159454, 519.1477453708649]</td>\n",
       "      <td>[858.5207009315491, 608.4342455863953]</td>\n",
       "      <td>[1283392, 621504, 1020280]</td>\n",
       "      <td>-0.587712</td>\n",
       "      <td>0.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239065</th>\n",
       "      <td>226206</td>\n",
       "      <td>-0.581809</td>\n",
       "      <td>-0.728975</td>\n",
       "      <td>0.850357</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.550581</td>\n",
       "      <td>0.408128</td>\n",
       "      <td>-1.431450</td>\n",
       "      <td>apical</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.8487598896026611, -0.4065110981464386, -0....</td>\n",
       "      <td>-0.984540</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>[1242320, 386568, 835506]</td>\n",
       "      <td>[1242730, 384648, 835346]</td>\n",
       "      <td>[823.984922170639, 584.9846363067627]</td>\n",
       "      <td>[948.8567638397216, 562.5469064712524]</td>\n",
       "      <td>[1267840, 625088, 983200]</td>\n",
       "      <td>0.328965</td>\n",
       "      <td>-0.442345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239066</th>\n",
       "      <td>227082</td>\n",
       "      <td>-1.233690</td>\n",
       "      <td>-0.560997</td>\n",
       "      <td>-0.682696</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.550581</td>\n",
       "      <td>-0.277208</td>\n",
       "      <td>-0.747283</td>\n",
       "      <td>basal</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.7182760238647461, 0.2712369859218597, -0.43...</td>\n",
       "      <td>0.653998</td>\n",
       "      <td>2.662264</td>\n",
       "      <td>[1307560, 720111, 821478]</td>\n",
       "      <td>[1307660, 721056, 822030]</td>\n",
       "      <td>[823.984922170639, 584.9846363067627]</td>\n",
       "      <td>[876.9663333892822, 475.53187251091]</td>\n",
       "      <td>[1267840, 625088, 983200]</td>\n",
       "      <td>-0.220960</td>\n",
       "      <td>-0.442345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239067</th>\n",
       "      <td>227102</td>\n",
       "      <td>-1.111118</td>\n",
       "      <td>-0.325588</td>\n",
       "      <td>0.167203</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.550581</td>\n",
       "      <td>-1.200803</td>\n",
       "      <td>-1.403504</td>\n",
       "      <td>basal</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.2488172650337219, -0.0253329630941152, -0.6...</td>\n",
       "      <td>0.660586</td>\n",
       "      <td>-1.209141</td>\n",
       "      <td>[1253340, 647535, 913773]</td>\n",
       "      <td>[1254210, 648092, 913497]</td>\n",
       "      <td>[823.984922170639, 584.9846363067627]</td>\n",
       "      <td>[840.3159034252167, 610.6111478805542]</td>\n",
       "      <td>[1267840, 625088, 983200]</td>\n",
       "      <td>-0.938165</td>\n",
       "      <td>-0.442345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239068</th>\n",
       "      <td>226434</td>\n",
       "      <td>-0.162126</td>\n",
       "      <td>-0.180224</td>\n",
       "      <td>0.585322</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.550581</td>\n",
       "      <td>0.765834</td>\n",
       "      <td>1.948422</td>\n",
       "      <td>basal</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.3395723104476929, -0.0049529541283845, -0.4...</td>\n",
       "      <td>0.827080</td>\n",
       "      <td>-0.676461</td>\n",
       "      <td>[1350760, 712929, 871647]</td>\n",
       "      <td>[1350490, 714924, 870280]</td>\n",
       "      <td>[823.984922170639, 584.9846363067627]</td>\n",
       "      <td>[870.390248298645, 547.6739072799683]</td>\n",
       "      <td>[1267840, 625088, 983200]</td>\n",
       "      <td>-0.387860</td>\n",
       "      <td>-0.442345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239069</th>\n",
       "      <td>226434</td>\n",
       "      <td>-0.162126</td>\n",
       "      <td>-0.180224</td>\n",
       "      <td>0.585322</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.550581</td>\n",
       "      <td>0.765834</td>\n",
       "      <td>1.948422</td>\n",
       "      <td>basal</td>\n",
       "      <td>RL</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.3395723104476929, -0.0049529541283845, -0.4...</td>\n",
       "      <td>0.827080</td>\n",
       "      <td>-0.676461</td>\n",
       "      <td>[1350760, 712929, 871647]</td>\n",
       "      <td>[1350490, 714924, 870280]</td>\n",
       "      <td>[823.984922170639, 584.9846363067627]</td>\n",
       "      <td>[870.390248298645, 547.6739072799683]</td>\n",
       "      <td>[1267840, 625088, 983200]</td>\n",
       "      <td>-0.387860</td>\n",
       "      <td>-0.442345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239070 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  adp_dist  post_skeletal_distance_to_soma  \\\n",
       "0        42593 -1.515697                        2.007164   \n",
       "1        42594 -1.225578                        1.070301   \n",
       "2        42595  0.632301                        2.099967   \n",
       "3        42596  0.645635                        1.931656   \n",
       "4        42597  1.334344                        1.668055   \n",
       "...        ...       ...                             ...   \n",
       "239065  226206 -0.581809                       -0.728975   \n",
       "239066  227082 -1.233690                       -0.560997   \n",
       "239067  227102 -1.111118                       -0.325588   \n",
       "239068  226434 -0.162126                       -0.180224   \n",
       "239069  226434 -0.162126                       -0.180224   \n",
       "\n",
       "        pre_skeletal_distance_to_soma  pre_oracle  pre_test_score  \\\n",
       "0                            0.931055    0.037561        0.787980   \n",
       "1                            0.268030    0.037561        0.787980   \n",
       "2                            0.905442    0.037561        0.787980   \n",
       "3                            0.956222    0.037561        0.787980   \n",
       "4                           -0.042977    0.037561        0.787980   \n",
       "...                               ...         ...             ...   \n",
       "239065                       0.850357    0.221486        0.550581   \n",
       "239066                      -0.682696    0.221486        0.550581   \n",
       "239067                       0.167203    0.221486        0.550581   \n",
       "239068                       0.585322    0.221486        0.550581   \n",
       "239069                       0.585322    0.221486        0.550581   \n",
       "\n",
       "        post_oracle  post_test_score compartment pre_brain_area  ...  \\\n",
       "0          1.191761        -0.205420        axon             RL  ...   \n",
       "1          1.191761        -0.205420        axon             RL  ...   \n",
       "2          1.191761        -0.205420        axon             RL  ...   \n",
       "3          1.191761        -0.205420        axon             RL  ...   \n",
       "4          1.191761        -0.205420        axon             RL  ...   \n",
       "...             ...              ...         ...            ...  ...   \n",
       "239065     0.408128        -1.431450      apical             RL  ...   \n",
       "239066    -0.277208        -0.747283       basal             RL  ...   \n",
       "239067    -1.200803        -1.403504       basal             RL  ...   \n",
       "239068     0.765834         1.948422       basal             RL  ...   \n",
       "239069     0.765834         1.948422       basal             RL  ...   \n",
       "\n",
       "                                    post_morph_embeddings  me_similarity  \\\n",
       "0       [1.0723994970321655, -0.7540942430496216, 0.11...      -0.934986   \n",
       "1       [1.0723994970321655, -0.7540942430496216, 0.11...      -0.934986   \n",
       "2       [1.0723994970321655, -0.7540942430496216, 0.11...      -0.934986   \n",
       "3       [1.0723994970321655, -0.7540942430496216, 0.11...      -0.934986   \n",
       "4       [1.0723994970321655, -0.7540942430496216, 0.11...      -0.934986   \n",
       "...                                                   ...            ...   \n",
       "239065  [-0.8487598896026611, -0.4065110981464386, -0....      -0.984540   \n",
       "239066  [0.7182760238647461, 0.2712369859218597, -0.43...       0.653998   \n",
       "239067  [0.2488172650337219, -0.0253329630941152, -0.6...       0.660586   \n",
       "239068  [1.3395723104476929, -0.0049529541283845, -0.4...       0.827080   \n",
       "239069  [1.3395723104476929, -0.0049529541283845, -0.4...       0.827080   \n",
       "\n",
       "        fw_similarity               axonal_coords            dendritic_coords  \\\n",
       "0           -0.182150  [1187660, 411978, 1089020]  [1187390, 412220, 1089160]   \n",
       "1           -0.182150   [1204580, 682542, 873138]   [1204640, 682870, 873890]   \n",
       "2           -0.182150  [1191790, 403683, 1093180]  [1188590, 402414, 1092660]   \n",
       "3           -0.182150  [1184320, 419286, 1082930]  [1186620, 419721, 1085540]   \n",
       "4           -0.182150   [1189150, 673302, 944202]   [1188790, 677771, 942901]   \n",
       "...               ...                         ...                         ...   \n",
       "239065       0.015884   [1242320, 386568, 835506]   [1242730, 384648, 835346]   \n",
       "239066       2.662264   [1307560, 720111, 821478]   [1307660, 721056, 822030]   \n",
       "239067      -1.209141   [1253340, 647535, 913773]   [1254210, 648092, 913497]   \n",
       "239068      -0.676461   [1350760, 712929, 871647]   [1350490, 714924, 870280]   \n",
       "239069      -0.676461   [1350760, 712929, 871647]   [1350490, 714924, 870280]   \n",
       "\n",
       "                                 pre_rf_coords  \\\n",
       "0       [882.7244675159454, 519.1477453708649]   \n",
       "1       [882.7244675159454, 519.1477453708649]   \n",
       "2       [882.7244675159454, 519.1477453708649]   \n",
       "3       [882.7244675159454, 519.1477453708649]   \n",
       "4       [882.7244675159454, 519.1477453708649]   \n",
       "...                                        ...   \n",
       "239065   [823.984922170639, 584.9846363067627]   \n",
       "239066   [823.984922170639, 584.9846363067627]   \n",
       "239067   [823.984922170639, 584.9846363067627]   \n",
       "239068   [823.984922170639, 584.9846363067627]   \n",
       "239069   [823.984922170639, 584.9846363067627]   \n",
       "\n",
       "                                post_rf_coords          pre_nucleus_coords  \\\n",
       "0       [858.5207009315491, 608.4342455863953]  [1283392, 621504, 1020280]   \n",
       "1       [858.5207009315491, 608.4342455863953]  [1283392, 621504, 1020280]   \n",
       "2       [858.5207009315491, 608.4342455863953]  [1283392, 621504, 1020280]   \n",
       "3       [858.5207009315491, 608.4342455863953]  [1283392, 621504, 1020280]   \n",
       "4       [858.5207009315491, 608.4342455863953]  [1283392, 621504, 1020280]   \n",
       "...                                        ...                         ...   \n",
       "239065  [948.8567638397216, 562.5469064712524]   [1267840, 625088, 983200]   \n",
       "239066    [876.9663333892822, 475.53187251091]   [1267840, 625088, 983200]   \n",
       "239067  [840.3159034252167, 610.6111478805542]   [1267840, 625088, 983200]   \n",
       "239068   [870.390248298645, 547.6739072799683]   [1267840, 625088, 983200]   \n",
       "239069   [870.390248298645, 547.6739072799683]   [1267840, 625088, 983200]   \n",
       "\n",
       "       nuclei_adp_dist feature_weight_398  \n",
       "0             0.076496           0.021513  \n",
       "1            -0.306546           0.021513  \n",
       "2             0.118612           0.021513  \n",
       "3             0.035778           0.021513  \n",
       "4            -0.587712           0.021513  \n",
       "...                ...                ...  \n",
       "239065        0.328965          -0.442345  \n",
       "239066       -0.220960          -0.442345  \n",
       "239067       -0.938165          -0.442345  \n",
       "239068       -0.387860          -0.442345  \n",
       "239069       -0.387860          -0.442345  \n",
       "\n",
       "[239070 rows x 29 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw = pd.read_csv(feature_path)\n",
    "fw = fw.drop(columns='nucleus_id')\n",
    "np.argmax(fw.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode All Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(column, df, suffix=''):\n",
    "    \"\"\"\n",
    "    one-hot encodes this shit\n",
    "    \"\"\"\n",
    "    cats = pd.unique(df[column])\n",
    "\n",
    "    for cat in cats:\n",
    "        new_col = cat+suffix\n",
    "        df[new_col] = df[column]==cat\n",
    "        df[new_col] = df[new_col].astype('int')\n",
    "    \n",
    "    df = df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode brain areas for all\n",
    "X_train_feat = one_hot('pre_brain_area', X_train_feat, '_pre')\n",
    "X_train_feat = one_hot('post_brain_area', X_train_feat, '_post')\n",
    "\n",
    "X_val_feat = one_hot('pre_brain_area', X_val_feat, '_pre')\n",
    "X_val_feat = one_hot('post_brain_area', X_val_feat, '_post')\n",
    "\n",
    "X_query_feat = one_hot('pre_brain_area', X_query_feat, '_pre')\n",
    "X_query_feat = one_hot('post_brain_area', X_query_feat, '_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode brain areas\n",
    "area1 = [\"basal\", \"soma\"]\n",
    "area2 = [\"axon\", \"apical\", \"oblique\", \"apical_shaft\"]\n",
    "area3 = [\"apical_tuft\"]\n",
    "\n",
    "def area_cols(df):\n",
    "    df[\"area1\"] = df[\"compartment\"].isin(area1).astype('int')\n",
    "    df[\"area2\"] = df[\"compartment\"].isin(area2).astype('int')\n",
    "    df[\"area3\"] = df[\"compartment\"].isin(area3).astype('int')\n",
    "    df.drop(columns='compartment')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = area_cols(X_train_feat)\n",
    "X_val_feat = area_cols(X_val_feat)\n",
    "X_query_feat = area_cols(X_query_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = X_train_feat.select_dtypes('number')\n",
    "X_val_feat = X_val_feat.select_dtypes('number')\n",
    "X_query_feat = X_query_feat.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                  int64\n",
       "adp_dist                          float64\n",
       "post_skeletal_distance_to_soma    float64\n",
       "pre_skeletal_distance_to_soma     float64\n",
       "pre_oracle                        float64\n",
       "pre_test_score                    float64\n",
       "post_oracle                       float64\n",
       "post_test_score                   float64\n",
       "post_nucleus_x                    float64\n",
       "post_nucleus_y                    float64\n",
       "post_nucleus_z                    float64\n",
       "pre_nucleus_id                      int64\n",
       "post_nucleus_id                     int64\n",
       "me_similarity                     float64\n",
       "fw_similarity                     float64\n",
       "nuclei_adp_dist                   float64\n",
       "AL_pre                              int64\n",
       "RL_pre                              int64\n",
       "V1_pre                              int64\n",
       "V1_post                             int64\n",
       "AL_post                             int64\n",
       "RL_post                             int64\n",
       "area1                               int64\n",
       "area2                               int64\n",
       "area3                               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_feat.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Lasso For Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_model = LogisticRegression(max_iter=500, solver='saga').fit(X_train_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7458810383129851"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fw_model.predict(X_val_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "balanced_accuracy_score(y_val,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_feats = pd.DataFrame({\"features\":X_val_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).columns})\n",
    "log_feats['coefs']=abs(fw_model.coef_[0])\n",
    "log_feats.sort_values(by='coefs', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_3 = ['adp_dist', 'post_skeletal_distance_to_soma', 'pre_skeletal_distance_to_soma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     post_test_score\n",
       "8      post_nucleus_y\n",
       "9      post_nucleus_z\n",
       "17            V1_post\n",
       "4      pre_test_score\n",
       "Name: features, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_feats['features'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     post_test_score\n",
       "8      post_nucleus_y\n",
       "9      post_nucleus_z\n",
       "17            V1_post\n",
       "4      pre_test_score\n",
       "Name: features, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_feats[\"features\"][0:5][~log_feats['features'][0:5].isin(known_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adp_dist',\n",
       " 'post_skeletal_distance_to_soma',\n",
       " 'pre_skeletal_distance_to_soma',\n",
       " 'post_test_score',\n",
       " 'post_nucleus_y',\n",
       " 'post_nucleus_z',\n",
       " 'V1_post',\n",
       " 'pre_test_score',\n",
       " 'fw_similarity']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_set = np.append(known_3, log_feats[\"features\"][0:6][~log_feats['features'][0:6].isin(known_3)])\n",
    "full_set = list(full_set)\n",
    "full_set.append('fw_similarity')\n",
    "# full_set.append('me_similarity')\n",
    "# full_set = known_3\n",
    "# full_set.append('V1_post')\n",
    "full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7268430863746624"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_model = LogisticRegression(max_iter=500, solver='saga').fit(X_train_feat[full_set].sort_index(axis=1), y_train)\n",
    "preds = fw_model.predict(X_query_feat[full_set].sort_index(axis=1))\n",
    "balanced_accuracy_score(y_query,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017671975408951"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_model = XGBClassifier(n_estimators=1, grow_policy='lossguide', max_leaves=0)\n",
    "xg_model.fit(X_train_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train)\n",
    "preds = xg_model.predict(X_val_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "balanced_accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5018827521747283"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_model = BaggingClassifier()\n",
    "bag_model.fit(X_train_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train)\n",
    "preds = bag_model.predict(X_val_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "balanced_accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Top Fucking Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(df, features, top):\n",
    "    \"\"\" \n",
    "    Function that selects the best features\n",
    "\n",
    "    Inputs:\n",
    "        - df: dataframe \n",
    "        - features: numpy array of features ranked by their importance as determined by different methods\n",
    "        - top: (int) number of top features to consider\n",
    "    \"\"\"\n",
    "    use_df = df.copy()\n",
    "    use_df = use_df[np.array(features[\"features\"][0:top])]\n",
    "\n",
    "    return use_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_use = feature_selector(X_train_feat, log_feats, 5)\n",
    "X_val_use = feature_selector(X_val_feat, log_feats, 5)\n",
    "X_query_use = feature_selector(X_query_feat, log_feats, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_test_score</th>\n",
       "      <th>post_nucleus_y</th>\n",
       "      <th>V1_post</th>\n",
       "      <th>post_nucleus_z</th>\n",
       "      <th>pre_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25710</th>\n",
       "      <td>-0.205420</td>\n",
       "      <td>-1.113624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>-0.980009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>-0.205420</td>\n",
       "      <td>-1.113624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>-0.980009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>-0.205420</td>\n",
       "      <td>-1.113624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>-0.980009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>-0.108867</td>\n",
       "      <td>-1.178216</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.581511</td>\n",
       "      <td>-0.980009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>-0.108867</td>\n",
       "      <td>-1.178216</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.581511</td>\n",
       "      <td>-0.980009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166059</th>\n",
       "      <td>-1.077152</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.741950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166060</th>\n",
       "      <td>-1.077152</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.741950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166061</th>\n",
       "      <td>-1.077152</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.741950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166062</th>\n",
       "      <td>-1.077152</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.741950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166063</th>\n",
       "      <td>-1.077152</td>\n",
       "      <td>-1.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.741950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40341 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_test_score  post_nucleus_y  V1_post  post_nucleus_z  \\\n",
       "25710         -0.205420       -1.113624        0        0.557703   \n",
       "25711         -0.205420       -1.113624        0        0.557703   \n",
       "25712         -0.205420       -1.113624        0        0.557703   \n",
       "25713         -0.108867       -1.178216        0       -1.581511   \n",
       "25714         -0.108867       -1.178216        0       -1.581511   \n",
       "...                 ...             ...      ...             ...   \n",
       "166059        -1.077152       -1.391026        0        0.466812   \n",
       "166060        -1.077152       -1.391026        0        0.466812   \n",
       "166061        -1.077152       -1.391026        0        0.466812   \n",
       "166062        -1.077152       -1.391026        0        0.466812   \n",
       "166063        -1.077152       -1.391026        0        0.466812   \n",
       "\n",
       "        pre_test_score  \n",
       "25710        -0.980009  \n",
       "25711        -0.980009  \n",
       "25712        -0.980009  \n",
       "25713        -0.980009  \n",
       "25714        -0.980009  \n",
       "...                ...  \n",
       "166059        0.741950  \n",
       "166060        0.741950  \n",
       "166061        0.741950  \n",
       "166062        0.741950  \n",
       "166063        0.741950  \n",
       "\n",
       "[40341 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_query_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if NOT feature selection\n",
    "X_train_use = X_train_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1)\n",
    "X_val_use = X_val_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1)\n",
    "X_query_use = X_query_feat.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this one out\n",
    "X_train_use =  X_train_feat[full_set]\n",
    "X_val_use =  X_val_feat[full_set]\n",
    "X_query_use =  X_query_feat[full_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\"RFC\": RandomForestClassifier(), \"LDA\": LinearDiscriminantAnalysis()}\n",
    "# param_grids = {\n",
    "#             \"RFC\": [{'n_estimators' : 1000, 'n_jobs' : -1}, \n",
    "#                     {'n_estimators' : 10000, 'n_jobs' : -1}],\n",
    "#             \"LDA\": [{'solver' : 'lsqr'},\n",
    "#                     {'solver' : 'eigen'}]\n",
    "#             }\n",
    "\n",
    "# OPTION 2 FOR LOGREG PERFORMED BEST ON FULL DATASET\n",
    "models = {\"XGBoost\": XGBClassifier(), 'LogReg':LogisticRegression()}\n",
    "param_grids = {\n",
    "            \"XGBoost\": [{'booster' : 'gbtree', 'eta':0.01, 'max_depth':10}, \n",
    "                    {'booster' : 'gblinear', 'eta':0.01, 'max_depth':10},\n",
    "                    {'booster' : 'gblinear', 'eta':0.05, 'max_depth':15}],\n",
    "        # \"LogReg\": [{'max_iter':500, 'penalty':'l2'},\n",
    "        #            {'max_iter':500, 'penalty':'l1', 'solver':'liblinear'},\n",
    "        #            {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.1, 'solver':'saga'},\n",
    "        #            {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.2, 'solver':'saga'},\n",
    "        #            {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.3, 'solver':'saga'},\n",
    "        #            {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.4, 'solver':'saga'},\n",
    "        #            {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.5, 'solver':'saga'}]\n",
    "            }\n",
    "\n",
    "# BEST: {'max_iter':500, 'penalty':'l2', 'solver':'newton-cholesky'} used on the top 5 features\n",
    "# models = {'LogReg':LogisticRegression()}\n",
    "# param_grids = {\n",
    "#         \"LogReg\": [{'max_iter':500, 'penalty':'l2', 'solver':'saga'},\n",
    "#                    {'max_iter':500, 'penalty':'l2', 'solver':'newton-cholesky'},\n",
    "#                    {'max_iter':500, 'penalty':'l1', 'solver':'saga'},\n",
    "#                    {'max_iter':500, 'penalty':'elasticnet', 'l1_ratio':0.6, 'solver':'saga'}]\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n",
      "[0 0 0 ... 0 0 0]\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "179187    False\n",
      "179188    False\n",
      "179189    False\n",
      "179190    False\n",
      "179191    False\n",
      "Name: connected, Length: 32518, dtype: bool\n",
      "[0.6662013772566536]\n",
      "0 0.6662013772566536\n",
      "XGBClassifier(base_score=None, booster='gblinear', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:11:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 0 0 0]\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "179187    False\n",
      "179188    False\n",
      "179189    False\n",
      "179190    False\n",
      "179191    False\n",
      "Name: connected, Length: 32518, dtype: bool\n",
      "[0.6662013772566536, 0.7325298791988968]\n",
      "0.6662013772566536 0.7325298791988968\n",
      "XGBClassifier(base_score=None, booster='gblinear', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eta=0.05, eval_metric=None,\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=15,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:11:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 0 0 0]\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "179187    False\n",
      "179188    False\n",
      "179189    False\n",
      "179190    False\n",
      "179191    False\n",
      "Name: connected, Length: 32518, dtype: bool\n",
      "[0.6662013772566536, 0.7325298791988968, 0.7234400599177702]\n"
     ]
    }
   ],
   "source": [
    "optimum_models = dict()\n",
    "accuracies = dict()\n",
    "for model in param_grids:\n",
    "    classifier = models[model]\n",
    "    prev_acc = 0\n",
    "    optimum_param = dict()\n",
    "    accuracy = []\n",
    "    for values in param_grids[model]:\n",
    "        \n",
    "        #Fitting to the training data with selected hyperparameters 0.732002483867241\n",
    "        classifier.set_params(**values)\n",
    "        print(classifier)\n",
    "        classifier.fit(X_train_use, y_train)\n",
    "\n",
    "        #Finding the balanced accuracy\n",
    "        y_hat = classifier.predict(X_query_use)\n",
    "        print(y_hat)\n",
    "        print(y_query)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_query, y_hat)\n",
    "        accuracy.append(balanced_accuracy)\n",
    "        print(accuracy)\n",
    "        if balanced_accuracy > prev_acc:\n",
    "            print(prev_acc, balanced_accuracy)\n",
    "            prev_acc = balanced_accuracy\n",
    "            optimum_param = values\n",
    "            \n",
    "        \n",
    "    accuracies[model] = accuracy\n",
    "    optimum_models[model] = classifier.set_params(**optimum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBoost': [0.6662013772566536, 0.7325298791988968, 0.7234400599177702]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=&#x27;gblinear&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=&#x27;gblinear&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster='gblinear', callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_models['XGBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make A Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used: Index(['adp_dist', 'post_skeletal_distance_to_soma',\n",
      "       'pre_skeletal_distance_to_soma', 'post_test_score', 'post_nucleus_y',\n",
      "       'post_nucleus_z', 'V1_post', 'pre_test_score', 'fw_similarity'],\n",
      "      dtype='object') \n",
      "Optimum Model Parameters: XGBClassifier(base_score=None, booster='gblinear', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n"
     ]
    }
   ],
   "source": [
    "# submission comment\n",
    "print('Features used:', X_train_use.columns, '\\nOptimum Model Parameters:', optimum_models['XGBoost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_path = \"../Data/leaderboard_data.csv\"\n",
    "sub_data = cleaner(leaderboard_path, feature_path, morph_path, submission = True)\n",
    "sub_data = area_cols(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'adp_dist', 'post_skeletal_distance_to_soma',\n",
       "       'pre_skeletal_distance_to_soma', 'pre_oracle', 'pre_test_score',\n",
       "       'post_oracle', 'post_test_score', 'compartment', 'pre_brain_area',\n",
       "       'post_brain_area', 'post_nucleus_x', 'post_nucleus_y', 'post_nucleus_z',\n",
       "       'pre_nucleus_id', 'post_nucleus_id', 'pre_feature_weights',\n",
       "       'post_feature_weights', 'pre_morph_embeddings', 'post_morph_embeddings',\n",
       "       'me_similarity', 'fw_similarity', 'axonal_coords', 'dendritic_coords',\n",
       "       'pre_rf_coords', 'post_rf_coords', 'pre_nucleus_coords',\n",
       "       'nuclei_adp_dist', 'area1', 'area2', 'area3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = one_hot('pre_brain_area', sub_data, '_pre')\n",
    "sub_data = one_hot('post_brain_area', sub_data, '_post')\n",
    "# sub_data = one_hot('projection_group', sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = np.append(known_3, log_feats[\"features\"][0:6][~log_feats['features'][0:6].isin(known_3)])\n",
    "full_set = list(full_set)\n",
    "full_set.append('fw_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adp_dist',\n",
       " 'post_skeletal_distance_to_soma',\n",
       " 'pre_skeletal_distance_to_soma',\n",
       " 'post_test_score',\n",
       " 'post_nucleus_y',\n",
       " 'post_nucleus_z',\n",
       " 'V1_post',\n",
       " 'pre_test_score',\n",
       " 'fw_similarity',\n",
       " 'ID']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_set = full_set\n",
    "sub_set.append('ID')\n",
    "sub_data_use = sub_data[full_set]\n",
    "full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/zl2jm8vs42b9q_gbk09_pdn40000gn/T/ipykernel_45666/2008015115.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_data_use[\"connected\"]=optimum_models['XGBoost'].predict(sub_data_use.drop(columns='ID'))\n"
     ]
    }
   ],
   "source": [
    "sub_data_use[\"connected\"]=optimum_models['XGBoost'].predict(sub_data_use.drop(columns='ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = sub_data_use.filter(['ID','connected'])\n",
    "submission_data['connected'] = submission_data['connected']==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.to_csv('submission_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7443735103690882"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = optimum_models['XGBoost'].predict(X_val_use)==1\n",
    "balanced_accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
